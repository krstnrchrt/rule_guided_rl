{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b711c41a",
   "metadata": {},
   "source": [
    "### 1. Create a mirror function for every rule that returns \n",
    "- as of now it returns binary variables. later, weights can be assigned and scores can be averaged\n",
    "- 1 = compliant, 0 = not-compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92d5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0914 15:56:26.447000 18892 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristina/anaconda3/envs/trl-env/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import regex as re\n",
    "\n",
    "from typing import List, Set, Generator, Dict, Any\n",
    "from spacy.tokens import Doc, Token\n",
    "from word2num_de import word_to_number\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import language_tool_python\n",
    "from sentence_transformers import SentenceTransformer, util, SimilarityFunction\n",
    "from sentence_transformers.evaluation import SimilarityFunction\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "from helper import SUBORDINATE_MARKERS, COORD_CONJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9a48c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file - german_dict/german_utf8.dic\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "from german_compound_splitter import comp_split\n",
    "\n",
    "utf8_file = os.path.join(\"german_dict\", \"german_utf8.dic\")\n",
    "ahoc = comp_split.read_dictionary_from_file(utf8_file) #activate the compound_spliter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538555a",
   "metadata": {},
   "source": [
    "## Generate Checker functions for compound and converted numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ac9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ok_numbers_converted(doc: Doc) -> float:\n",
    "    \"\"\"\n",
    "    A single, self-contained function to check for unconverted numbers.\n",
    "    This has NO external dependencies to ensure the correct logic is always executed.\n",
    "    \"\"\"\n",
    "    # All necessary constants are defined INSIDE this function\n",
    "    NUMBER_DICT = {\n",
    "    # Ordinals\n",
    "    \"erster\": \"1.\", \"zweiter\": \"2.\", \"dritter\": \"3.\", \"vierter\": \"4.\", \"fünfter\": \"5.\", \"sechster\": \"6.\", \"siebter\": \"7.\",\n",
    "    \"achter\": \"8.\", \"neunter\": \"9.\", \"zehnter\": \"10.\", \"elfter\": \"11.\", \"zwölfter\": \"12.\",\n",
    "    # Fractions\n",
    "    \"halb\": \"0.5\", \"eineinhalb\": \"1.5\", \"zweieinhalb\": \"2.5\", \"dreieinhalb\": \"3.5\", \"viereinhalb\": \"4.5\",\n",
    "    \"fünfeinhalb\": \"5.5\", \"sechseinhalb\": \"6.5\", \"siebeneinhalb\": \"7.5\", \"achteinhalb\": \"8.5\", \"neuneinhalb\": \"9.5\", \"zehneinhalb\": \"10.5\",\n",
    "}\n",
    "    RE_NUMERIC = re.compile(r\"^\\d+([.,]\\d+)?$\")\n",
    "    RE_ORDINAL = re.compile(r\"^\\d+\\.$\")\n",
    "\n",
    "    # --- Internal helper to check each token\n",
    "\n",
    "    def _is_unconverted_internal(token: Token) -> bool:\n",
    "        \"\"\"Internal helper to check a single token.\"\"\"\n",
    "        # This is the 'like_num' logic, using token attributes correctly\n",
    "        text, lemma = token.text, token.lemma_\n",
    "        text_lower, lemma_lower = text.lower(), lemma.lower()\n",
    "        is_like_num = False\n",
    "        if lemma_lower in NUMBER_DICT or text_lower in NUMBER_DICT: is_like_num = True\n",
    "        elif RE_NUMERIC.match(text) or RE_ORDINAL.match(text): is_like_num = True\n",
    "        elif text.isdigit(): is_like_num = True\n",
    "        else:\n",
    "            try:\n",
    "                word_to_number(lemma_lower)\n",
    "                is_like_num = True\n",
    "            except Exception:\n",
    "                try:\n",
    "                    word_to_number(text_lower)\n",
    "                    is_like_num = True\n",
    "                except Exception:\n",
    "                    is_like_num = False\n",
    "        \n",
    "        # 'is_number' logic\n",
    "        is_a_number = False\n",
    "        if token.text.lower() == \"ein\" and token.pos_ != \"NUM\":\n",
    "            is_a_number = False\n",
    "        else:\n",
    "            is_a_number = is_like_num or token.pos_ == \"NUM\"\n",
    "\n",
    "        # 'is_number_word_that_should_be_converted' logic\n",
    "        if not is_a_number:\n",
    "            return False\n",
    "        return not token.text.isdigit()\n",
    "\n",
    "    # --- Main calculation\n",
    "    violating_tokens = [token for token in doc if _is_unconverted_internal(token)]\n",
    "    violation_count = len(violating_tokens)\n",
    "    total_tokens = len(doc)\n",
    "    \n",
    "    score = 1.0 # Default score is perfect (1.0)\n",
    "    if total_tokens > 0: # Calculate penalty based on violations\n",
    "        penalty = min(1.0, violation_count / total_tokens) # Normalize the penalty\n",
    "        score = 1.0 - penalty\n",
    "\n",
    "    return score # if I want to return all for tracking/debugging violation_count, total_tokens, violating_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d3dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_unsplit_compound(doc: spacy.tokens.Doc, ahoc: set) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a document contains any unsplit compound nouns\n",
    "    that should be simplified according to the given rules.\n",
    "\n",
    "    This function iterates through each token in a spaCy Doc object and\n",
    "    applies a set of heuristics to determine if it is a compound that\n",
    "    should have been split but wasn't.\n",
    "\n",
    "    Args:\n",
    "        doc (spacy.tokens.Doc): The spaCy document object to check.\n",
    "        ahoc (set): A lexicon or set of valid German words for checking\n",
    "                    the validity of split parts.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if at least one unsplit compound is found, False otherwise.\n",
    "    \"\"\"\n",
    "    for token in doc:\n",
    "        # Step 1: Preliminary checks on the token based on your logic.\n",
    "        # This combines the logic from your `check_compound_split` and\n",
    "        # `should_split` functions.\n",
    "        if token.pos_ != \"NOUN\" or token.ent_type_ in {\"PER\", \"LOC\", \"ORG\"}:\n",
    "            continue\n",
    "\n",
    "        # Step 2: Attempt to split the compound using your splitter.\n",
    "        parts =  comp_split.dissect(token.text, ahoc)\n",
    "        \n",
    "        # Step 3: Check if the token is a compound that can be split.\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Step 4: Apply your \"Konvens\" rule check.\n",
    "        # This rule suggests that if both the first and last parts of a\n",
    "        # compound are short (<= 4 characters), it's not considered\n",
    "        # \"hard to read\" and shouldn't be flagged as a violation.\n",
    "        if len(parts[0]) <= 4 and len(parts[-1]) <= 4:\n",
    "            continue\n",
    "\n",
    "        # Step 5: Check if the split parts are valid words in the lexicon.\n",
    "        # This ensures we don't try to split non-compounds or proper nouns\n",
    "        # that aren't marked as named entities.\n",
    "        valid_parts_count = sum(p.lower() in ahoc for p in parts)\n",
    "        \n",
    "        # Step 6: If a majority of the parts are valid, it's a compound that\n",
    "        # should have been split. We've found a violation.\n",
    "        if valid_parts_count / len(parts) >= 0.8:\n",
    "            # Processing Optimization Changes\n",
    "            # print(f\"Violation detected: Found unsplit compound '{token.text}'\")\n",
    "            return True\n",
    "            \n",
    "    # If the loop completes without finding any violations, the rule is followed.\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1589545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_apposition(doc: spacy.tokens.Doc) -> bool: #regex finds likely comma apposition\n",
    "    if any(tok.dep_ == \"app\" for tok in doc):\n",
    "        return True\n",
    "    # Fallback: regex check for ', ... ,'\n",
    "    # Only trigger if pattern matches (not followed by \"die\", \"der\", etc.)\n",
    "    match = re.search(r', (?!die |der |das |und |aber |weil |obwohl )[^,]+,', doc.text)\n",
    "    return bool(match)\n",
    "\n",
    "def has_subordinate_clause(doc: spacy.tokens.Doc) -> bool:\n",
    "    for tok in doc:\n",
    "        if (tok.text.lower() in SUBORDINATE_MARKERS and \n",
    "            (tok.dep_ in \"cp\" or tok.pos_ == \"SCONJ\")):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_coordinate_clause(doc: spacy.tokens.Doc) -> bool:\n",
    "    \"\"\"Check if the document contains a coordinate clause.\"\"\"\n",
    "    return any(tok.dep_ == \"cd\" and tok.text.lower() in COORD_CONJ for tok in doc)\n",
    "\n",
    "def has_disallowed_tense(doc: spacy.tokens.Doc) -> bool:\n",
    "    for tok in doc:\n",
    "        if tok.pos_ in (\"VERB\", \"AUX\"):\n",
    "            tense = tok.morph.get(\"Tense\")\n",
    "            form = tok.morph.get(\"VerbForm\")\n",
    "            mood = tok.morph.get(\"Mood\")\n",
    "            if (\"Pres\" not in tense and \"Part\" not in form) or (\"Sub\" in mood):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def is_passive(doc: spacy.tokens.Doc) -> bool:\n",
    "    # Werden + participle: Vorgangspassiv (event passive)\n",
    "    has_werden = any(tok.lemma_ == \"werden\" and tok.pos_ == \"AUX\" for tok in doc)\n",
    "    has_participle = any(tok.pos_ == \"VERB\" and \"Part\" in tok.morph.get(\"VerbForm\", []) for tok in doc)\n",
    "    if has_werden and has_participle:\n",
    "        return True\n",
    "    # Sein + participle: Zustandspassiv (state passive), only for transitive verbs!\n",
    "    has_sein = any(tok.lemma_ == \"sein\" and tok.pos_ == \"AUX\" for tok in doc)\n",
    "    if has_sein and has_participle:\n",
    "        # Check: is the main verb transitive (does it take an object)?\n",
    "        if any(tok.dep_ in {\"oa\", \"obj\"} for tok in doc):  # object present\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e088b",
   "metadata": {},
   "source": [
    "# Collect the main functions to compute reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e95e48",
   "metadata": {},
   "source": [
    "### Example Sentence\n",
    "\n",
    "- has_apposition(doc) returns True/False\n",
    "- float(has_apposition(doc)) returns 1.0 (True, violated), 0.0 (False, compliant)\n",
    "- what does the ok counterpart do?\n",
    "  - flips the perspective\n",
    "\n",
    "| original text flag | has_xy -> float | ok_no_xy | meaning |\n",
    "| --- | --- | --- | --- |\n",
    "| apposition present | 1.0 | 0.0 | not okay |\n",
    "| no apposition | 0.0 | 1.0 | okay |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c73dec",
   "metadata": {},
   "source": [
    "| Function | Returns | Meaning |\n",
    "| --- | --- | --- |\n",
    "| float(has_xy(...)) | 1.0 if violation, 0.0 if compliant | \"Badness Score\"|\n",
    "| ok_no_xy | 1.0 if compliant, 0.0 if violation | \"Goodness Score\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a66af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ok_no_apposition(doc)           -> float: return 1.0 - float(has_apposition(doc))\n",
    "def ok_no_subordinate_clause(doc)   -> float: return 1.0 - float(has_subordinate_clause(doc))\n",
    "#def ok_no_coordinate_clause(doc)    -> float: return 1.0 - float(has_coordinate_clause(doc))\n",
    "def ok_active_voice(doc)            -> float: return 1.0 - float(is_passive(doc))\n",
    "def ok_allowed_verb_tense(doc)      -> float: return 1.0 - float(has_disallowed_tense(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a single \"source of truth\" generator function\n",
    "def _find_unsplit_compounds_gen(doc: spacy.tokens.Doc, ahoc: Set[str]) -> Generator[spacy.tokens.Token, None, None]:\n",
    "    \"\"\"\n",
    "    A generator that yields each token that is an unsplit compound violation.\n",
    "    This contains the core, shared logic.\n",
    "    \"\"\"\n",
    "    for token in doc:\n",
    "        # Initial checks: must be a NOUN and not a named entity\n",
    "        if token.pos_ != \"NOUN\" or token.ent_type_ in {\"PER\", \"LOC\", \"ORG\"}:\n",
    "            continue\n",
    "        if not token.text or not token.text.strip() or not any(char.isalpha() for char in token.text): #adding fix\n",
    "            continue\n",
    "\n",
    "        \n",
    "        parts = [] # Default to an empty list\n",
    "        try:\n",
    "            parts = comp_split.dissect(token.text, ahoc)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        \n",
    "        if not parts or len(parts) < 2:\n",
    "            continue\n",
    "        if len(parts[0]) <= 4 and len(parts[-1]) <= 4:\n",
    "            continue\n",
    "\n",
    "        valid_parts_count = sum(p.lower() in ahoc for p in parts)\n",
    "        \n",
    "        if valid_parts_count / len(parts) >= 0.8:\n",
    "            yield token # Yield the violating token and continue the loop\n",
    "\n",
    "def count_unsplit_compounds(doc: spacy.tokens.Doc, ahoc: Set[str]) -> int:\n",
    "    \"\"\"Counts ALL unsplit compounds\"\"\"\n",
    "    return sum(1 for _ in _find_unsplit_compounds_gen(doc, ahoc))\n",
    "\n",
    "def find_all_unsplit_compounds(doc: spacy.tokens.Doc, ahoc: Set[str]) -> list[spacy.tokens.Token]:\n",
    "    \"\"\"Gets a list of all violating tokens. Useful for debugging.\"\"\"\n",
    "    return list(_find_unsplit_compounds_gen(doc, ahoc))\n",
    "\n",
    "def ok_no_compounds(doc: spacy.tokens.Doc) -> float:\n",
    "    \"\"\"\n",
    "    Computes a compliance score (0-1) for compound splitting.\n",
    "    \"\"\"\n",
    "    violation_count = count_unsplit_compounds(doc, ahoc)\n",
    "    noun_count = len([token for token in doc if token.pos_ == \"NOUN\"])\n",
    "    \n",
    "    if noun_count == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    penalty = min(1.0, violation_count / noun_count)\n",
    "    return 1.0 - penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c831eb4",
   "metadata": {},
   "source": [
    "### 2. Collect them all in a single rule wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28009e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RULE_CHECKS = {\n",
    "    \"apposition\"        : ok_no_apposition,\n",
    "    \"subord_clause\"     : ok_no_subordinate_clause,\n",
    "    #\"coord_clause\"      : ok_no_coordinate_clause,\n",
    "    \"voice_active\"      : ok_active_voice,\n",
    "    \"verb_tense\"        : ok_allowed_verb_tense,\n",
    "    \"no_compounds\"      : ok_no_compounds,\n",
    "    \"numbers_converted\" : ok_numbers_converted,\n",
    "}\n",
    "RULE_WEIGHTS = { # must sum to 1.0\n",
    "   \"apposition\"        : 0.20,   \n",
    "   \"subord_clause\"     : 0.10,   \n",
    "   \"voice_active\"      : 0.15,   \n",
    "   \"verb_tense\"        : 0.25,   \n",
    "   \"no_compounds\"      : 0.20,   \n",
    "   \"numbers_converted\" : 0.10,   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize weights to ensure they sum to 1.0\n",
    "total_weight = sum(RULE_WEIGHTS.values())\n",
    "if total_weight != 1.0:\n",
    "    for key in RULE_WEIGHTS:\n",
    "        RULE_WEIGHTS[key] /= total_weight\n",
    "    print(\"Warning: RULE_WEIGHTS did not sum to 1.0. They have been normalized.\")\n",
    "print(total_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77edd1ac",
   "metadata": {},
   "source": [
    "### 3. Create the scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60cbb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_compliance_score(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Computes a weighted 0-1 compliance score for a simplified sentence.\n",
    "    This is the core function for rule-based rewards.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    total_score = 0.0\n",
    "    \n",
    "    # Iterate through each rule and its corresponding checker function\n",
    "    for name, check_func in RULE_CHECKS.items():\n",
    "        # Call the checker function and get the score\n",
    "        score = check_func(doc)\n",
    "        # Clamp score for required range\n",
    "        score = max(0.0, min(1.0, score))\n",
    "        \n",
    "        # Add the weighted score to the total\n",
    "        total_score += RULE_WEIGHTS[name] * score\n",
    "        \n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03997b8",
   "metadata": {},
   "source": [
    "### 4. Compute a meaning-preservation score\n",
    "- embedding-level similarity which works cross-lingually without refences\n",
    "- https://sbert.net/docs/sentence_transformer/usage/semantic_textual_similarity.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656aa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name deepset/gbert-base. Creating a new one with mean pooling.\n",
      "/Users/kristina/anaconda3/envs/trl-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model similarity function set to: 'dot'\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading BERT model...\")\n",
    "model = SentenceTransformer('deepset/gbert-large') #-large -small\n",
    "model.similarity_fn_name = SimilarityFunction.DOT_PRODUCT #dot product is faster and more efficient\n",
    "print(f\"Model similarity function set to: '{model.similarity_fn_name}'\")\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe4191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_semantic_similarity(original_sentence: str, simplified_sentence: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the meaning preservation score using SBERT embeddings.\n",
    "    Returns a score between 0.0 and 1.0.\n",
    "    \"\"\"\n",
    "    # Encode sentences with normalization for faster comparison\n",
    "    emb_original = model.encode(original_sentence, normalize_embeddings=True)\n",
    "    emb_simplified = model.encode(simplified_sentence, normalize_embeddings=True)\n",
    "    \n",
    "    \n",
    "    # 2. Calculate the cosine similarity between the two vectors. This returns a tensor.\n",
    "    similarity_score = model.similarity(emb_original, emb_simplified).item()\n",
    "    \n",
    "    #map the [-1,1] range to [0,1]\n",
    "    return (similarity_score + 1.0) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5bb57f",
   "metadata": {},
   "source": [
    "# Compute Grammar Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c83cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = language_tool_python.LanguageTool('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b411ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grammar_score(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Computes a grammatical compliance score (0-1) for a sentence.\n",
    "    A score of 1.0 means no errors were found.\n",
    "    \"\"\"\n",
    "    error_count = 0\n",
    "    \n",
    "    if not text.strip():\n",
    "        return 1.0 # Return perfect score for empty strings\n",
    "    \n",
    "    # Check the text for errors\n",
    "    matches = tool.check(text)\n",
    "\n",
    "    grammar_errors = [match for match in matches if match.category == 'GRAMMAR']\n",
    "\n",
    "\n",
    "    if matches:\n",
    "        #print(f\"Found {len(grammar_errors)} grammar errors\")\n",
    "        error_count = len(grammar_errors) #track number of errors\n",
    "        #for match in matches:\n",
    "        #    print(f\"Error: {match.message}\")\n",
    "    \n",
    "    # Get a base for normalization, by counting number of tokens\n",
    "    token_count = len(text.split())\n",
    "    \n",
    "    if token_count == 0:\n",
    "        return 1.0\n",
    "        \n",
    "    # Calculate the penalty. I use a simple normalization. Capped at 1. Make sure to be less harsh on short sentences\n",
    "    #penalty = min(1.0, error_count / token_count) ## linear penalty\n",
    "    penalty = error_count / (error_count + token_count) #softer penalty, as errors increase, penalty -> 1.0 asymptotically\n",
    "    \n",
    "    # Return the compliance score\n",
    "    return 1.0 - penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c95190b",
   "metadata": {},
   "source": [
    "### 4. Plug into the global reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da4d95",
   "metadata": {},
   "source": [
    "Assessing four different weight distribution options:\n",
    "\n",
    "Baseline (0.5, 0.3, 0.2)\n",
    "\n",
    "Rules-heavy (0.6, 0.2, 0.2)\n",
    "\n",
    "Meaning-heavy (0.3, 0.6, 0.1)\n",
    "\n",
    "Grammar-sensitive (0.4, 0.2, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b003e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = [\n",
    "#     {\"rules_score\": 0.5, \"meaning_score\": 0.3, \"grammar_score\": 0.2},\n",
    "#     {\"rules_score\": 0.6, \"meaning_score\": 0.2, \"grammar_score\": 0.2},\n",
    "#     {\"rules_score\": 0.3, \"meaning_score\": 0.6, \"grammar_score\": 0.1},\n",
    "#     {\"rules_score\": 0.4, \"meaning_score\": 0.2, \"grammar_score\": 0.4},\n",
    "# ]\n",
    "\n",
    "# weights = {\n",
    "#     \"rules_score\":   0.5,\n",
    "#     \"meaning_score\": 0.3,\n",
    "#     \"grammar_score\": 0.2,\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d4e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(src: str, simpl: str, weights: dict) -> float:\n",
    "    \"\"\"\n",
    "    Combines the rule compliance, meaning preservation and grammar scores into one.\n",
    "    \n",
    "    Args:\n",
    "        src (str): The original, complex sentence.\n",
    "        simpl (str): The simplified output sentence.\n",
    "    \n",
    "    Returns:\n",
    "        float: The final reward score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure weights sum is very close avoiding floating point issues\n",
    "    if abs(sum(weights.values()) - 1.0) > 1e-9:\n",
    "        \n",
    "        raise ValueError(f\"Reward weights must sum to 1.0, but they sum to {sum(weights.values())}\")\n",
    "    \n",
    "    \n",
    "    r_rules   = rule_compliance_score(simpl)\n",
    "    r_meaning = calculate_semantic_similarity(src, simpl)\n",
    "    r_grammar = calculate_grammar_score(simpl)\n",
    "\n",
    "    reward = (weights[\"rules_score\"]   * r_rules +\n",
    "              weights[\"meaning_score\"] * r_meaning +\n",
    "              weights[\"grammar_score\"] * r_grammar)\n",
    "    \n",
    "    # Clamp final combined reward\n",
    "    return float(max(0.0, min(1.0, reward)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
