{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be484354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from trl import PPOTrainer\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66edef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "\n",
    "# --- 1. Set the Input Policy Model ---\n",
    "# For direct PPO (Experiments 1A & 1B), use the base model ID.\n",
    "POLICY_MODEL_ID = \"frhew/sigdial_ft_a2\" \n",
    "# For SFT-enhanced PPO (Experiments 2A & 2B), comment the line above and use \n",
    "# the SFT model output directory instead.\n",
    "# POLICY_MODEL_ID = SFT_MODEL_OUTPUT_DIR \n",
    "\n",
    "\n",
    "# --- 2. Define the Instruction Prompt Template ---\n",
    "def format_query(example):\n",
    "    \"\"\"Applies the chosen instruction template to the input sentence.\"\"\"\n",
    "    \n",
    "    # DETAILED PROMPT (for Experiments 1B, 2A, 2B)\n",
    "    # Provides more context and explicitly mentions \"Leichter-Sprache-Regeln\".\n",
    "    example['query'] = f\"Aufgabe: Vereinfache den folgenden Satz nach Leichter-Sprache-Regeln.\\nSatz: {example['original']}\"\n",
    "    \n",
    "    # SIMPLE PROMPT (for Experiment 1A)\n",
    "    # A concise, no-context instruction. Uncomment the line below to use it.\n",
    "    # example['query'] = f\"Vereinfache diesen Satz: {example['query']}\"\n",
    "    \n",
    "    return example\n",
    "\n",
    "\n",
    "# --- 3. Define the Output Model Name ---\n",
    "# Use a descriptive name for the final PPO model saved after training.\n",
    "MODEL_FILE_NAME = \"PPO_model_2K_4E_28\"\n",
    "\n",
    "\n",
    "# --- 4. Set Training Parameters ---\n",
    "DATASET_SIZE = 2048\n",
    "EPOCH = 4\n",
    "\n",
    "\n",
    "# --- 5. Verify Data Paths ---\n",
    "# Ensure these paths point to your split dataset files.\n",
    "MASTER_TRAIN_DATASET = \"sft_split_dataset/train.csv\"\n",
    "MASTER_EVAL_DATASET = \"sft_split_dataset/eval.csv\"\n",
    "MASTER_TEST_DATASET = \"sft_split_dataset/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef9960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Model Paths ---\n",
    "\n",
    "## Are you implementing direct PPO training (1) or with SFT in-between (2)\n",
    "\n",
    "# The policy model is the base LLM we want to fine-tune with PPO.\n",
    "POLICY_MODEL_ID = \"frhew/sigdial_ft_a2\" # --- (1) directly pluck in model\n",
    "\n",
    "### IF YOU'RE USING SFT MODEL, UNCOMMENT THE NEXT TWO LINES --- (2)\n",
    "#SFT_MODEL_OUTPUT_DIR = \"my_sft_tuned_model_v1\"\n",
    "#POLICY_MODEL_ID = SFT_MODEL_OUTPUT_DIR\n",
    "\n",
    "# Define the pre-trained reward model here.\n",
    "RM_PATH = \"rm_out_rules_heavy_final\"\n",
    "\n",
    "\n",
    "\n",
    "# --- Load Policy and Reference Models ---\n",
    "# The policy model is loaded with a value head for PPO training.\n",
    "# policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(POLICY_MODEL_ID)\n",
    "# the above is loaded LATER with lora -- IGNORE\n",
    "\n",
    "# The reference model is a frozen copy of the original policy.\n",
    "# --- Load the standard reference model (without LoRA) ---\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "                POLICY_MODEL_ID,\n",
    "                torch_dtype=torch.bfloat16, # <-- Use lower precision\n",
    ")\n",
    "\n",
    "# the above line takes 30min tot downlaod 7 shareds each close to 5GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b926beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Tokenizers ---\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(POLICY_MODEL_ID, padding_side='left') # Add padding_side\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(RM_PATH, padding_side='left')     # Add padding_side\n",
    "\n",
    "print(\"All models and tokenizers loaded successfully.\")\n",
    "\n",
    "# Set padding token for both tokenizers if it's not already set.\n",
    "#TODO assess which padding is required \n",
    "if policy_tokenizer.pad_token is None:\n",
    "    policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "if reward_tokenizer.pad_token is None:\n",
    "    reward_tokenizer.pad_token = reward_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d079934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Custom Reward Model ---\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(RM_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099603bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configure Hardware Device ---\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8236d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the reward model to the correct device and set it to evaluation mode.\n",
    "reward_model.to(device) # 'device' was set to \"mps\" in a previous cell\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff5c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  2. PREPARE DATASET FOR PPO\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# Load the dataset containing the prompts for the policy model.\n",
    "df = pd.read_csv(MASTER_TRAIN_DATASET, index_col=0)\n",
    "train_df = df\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "ppo_dataset = Dataset.from_pandas(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba78512",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defined in the beginning (Configuration Step)\n",
    "# Apply the formatting function to every example in the dataset.\n",
    "# The .map() function processes each row and updates the 'query' column.\n",
    "###applying this on EVAL happens later on\n",
    "ppo_dataset = ppo_dataset.map(format_query)\n",
    "\n",
    "# You can now check an example to see the new format\n",
    "print(\"Example of a formatted query:\")\n",
    "print(ppo_dataset[0]['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53777f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Hugging Face Dataset from a smaller, random sample.\n",
    "# Using 1024-2048 prompts is often sufficient for a PPO run.\n",
    "# Using a smaller, diverse set of prompts is often sufficient to train a robust policy, \n",
    "# as it provides enough variety to generate the experiences the model needs to learn from, \n",
    "# without the massive memory cost.\n",
    "#4,096, 6,144\n",
    "ppo_dataset = Dataset.from_pandas(train_df.sample(n=DATASET_SIZE, random_state=42))\n",
    "\n",
    "def tokenize_query(examples):\n",
    "    \"\"\"Tokenizes the 'query' column for the PPO trainer.\"\"\"\n",
    "    # Remove `padding=\"max_length\"`. The PPOTrainer's dataloader will handle padding dynamically.\n",
    "    #return policy_tokenizer(examples[\"query\"], truncation=True, max_length=60)\n",
    "    # \"\"\"Tokenizes the 'query' column for the PPO trainer.\"\"\"\n",
    "    return policy_tokenizer(examples[\"query\"], \n",
    "                            truncation=True, \n",
    "                            padding=\"max_length\", \n",
    "                            max_length=40)\n",
    "\n",
    "# Tokenize the dataset and format it for PyTorch.\n",
    "ppo_dataset = ppo_dataset.map(tokenize_query, batched=True,\n",
    "                              remove_columns=['final_simplification', 'applied_rules', 'uid', 'query'])\n",
    "ppo_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14368661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  3. CONFIGURE PPO (WITH LORA)\n",
    "# ==============================================================================\n",
    "\n",
    "from trl import PPOConfig, PPOTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "# --- Define the LoRA configuration ---\n",
    "# This object defines the configuration for LoRA (Low-Rank Adaptation),\n",
    "# a technique for efficient model fine-tuning.\n",
    "lora_config = LoraConfig(\n",
    "    # r: The rank of the LoRA matrices. This is the most important parameter.\n",
    "    # It controls the number of trainable parameters. A higher rank means more\n",
    "    # expressive power but also more parameters to train. Common values are 8, 16, 32.\n",
    "    r=16,\n",
    "    \n",
    "    # lora_alpha: A scaling factor for the LoRA updates. It's like a learning\n",
    "    # rate for the adapter layers. A common practice is to set alpha to twice the rank (2 * r).\n",
    "    lora_alpha=32,\n",
    "    \n",
    "    # lora_dropout: Applies dropout regularization to the LoRA layers. This helps\n",
    "    # prevent overfitting by randomly setting a fraction of adapter activations to zero.\n",
    "    lora_dropout=0.05,\n",
    "    \n",
    "    # bias: Determines which bias parameters in the model are trained. \"none\" is a\n",
    "    # common setting that freezes all bias terms and only trains the new LoRA weights.\n",
    "    bias=\"none\",\n",
    "    \n",
    "    # task_type: Specifies the type of model you are adapting. This is crucial for\n",
    "    # the PEFT library to correctly identify and modify the right layers.\n",
    "    # \"CAUSAL_LM\" is correct for GPT-style models used for text generation.\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99795051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Load the Policy Model with PEFT config ---\n",
    "# # TRL's special model class can directly accept a peft_config.\n",
    "# # This ensures the model is created in the exact format the PPOTrainer expects.\n",
    "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    POLICY_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16, # <-- Use lower precision\n",
    "    peft_config=lora_config  # <-- Pass the LoRA config here during loading\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to trade a little computation for a lot of memory.\n",
    "#policy_model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e4292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the PPOConfig  ---\n",
    "# A recommended, stable set of hyperparameters for a single run without a full search.\n",
    "config = PPOConfig(\n",
    "    model_name=POLICY_MODEL_ID,\n",
    "    # A learning rate of 1.4e-5 is a common and effective starting point for fine-tuning.\n",
    "    learning_rate=1.4e-5,\n",
    "    # The number of times you iterate over the collected PPO experiences in each optimization step.\n",
    "    ppo_epochs= EPOCH, #default is 4\n",
    "    # The number of prompts to collect before performing an optimization.\n",
    "    batch_size=16, #32,\n",
    "    # The PPO batch is split into smaller mini-batches for the update.\n",
    "    mini_batch_size=4, #8,\n",
    "\n",
    "    # --- MEMORY FIX 2: Add Gradient Accumulation ---\n",
    "    # This will process 4 mini-batches before performing a model update.\n",
    "    # Effective Batch Size = 4 (mini_batch_size) * 4 (accumulation_steps) = 16\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    # Disables external logging integrations like WandB.\n",
    "    log_with=None,\n",
    "    ### --- FIX\n",
    "    # Use the full KL penalty to ensure stability with LoRA-adapted modles. This prevents the negative KL divergence by applying the KL calculation more robustly.\n",
    "    # Use a KL penalty to stop the model from deviating too far from the original.\n",
    "    kl_penalty=\"full\", #changed frol kl\n",
    "    # A slightly lower target KL can improve stability and prevent the model from changing too drastically.\n",
    "    target_kl= 0.05,\n",
    "    # --- Recommended additions for stability ---\n",
    "    # Normalizes the reward scores, which is a key practice for stable PPO training.\n",
    "    use_score_scaling=True,\n",
    "    # The coefficient for the value function loss in the PPO update.\n",
    "    vf_coef=0.1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7ffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize the PPOTrainer ---\n",
    "# Initialize the PPOTrainer with all our components.\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=config,\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=policy_tokenizer,\n",
    "    dataset=ppo_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  4. THE PPO TRAINING LOOP\n",
    "# ==============================================================================\n",
    "from trl import PPOTrainer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Generation settings for creating the simplified responses.\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": policy_tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 40, # Control the length of the simplification\n",
    "}\n",
    "\n",
    "mean_rewards = []\n",
    "\n",
    "# The main training loop\n",
    "for epoch in range(config.ppo_epochs):\n",
    "    #for batch in tqdm(ppo_trainer.dataloader, f\"PPO Epoch {epoch+1}\"):\n",
    "    for i, batch in tqdm(enumerate(ppo_trainer.dataloader), f\"PPO Epoch {epoch+1}\"):\n",
    "        \n",
    "        # A. Get prompts (queries) from the batch.\n",
    "        query_tensors = batch['input_ids']\n",
    "\n",
    "        # B. Generate responses from the policy model.\n",
    "        # THE FIX: Convert the 2D batch tensor into a list of 1D tensors.\n",
    "        queries_list = [q for q in query_tensors]\n",
    "        response_tensors = ppo_trainer.generate(queries_list, **generation_kwargs)\n",
    "        batch['response'] = policy_tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "\n",
    "        # C. Score the responses with your custom reward model.\n",
    "        # This is where the \"Feedback\" from your diagram happens.\n",
    "        texts_to_score = batch['response']\n",
    "        rewards = []\n",
    "        with torch.no_grad():\n",
    "            # Tokenize for the reward model\n",
    "            inputs = reward_tokenizer(texts_to_score, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            # Get the raw score (logits)\n",
    "            reward_logits = reward_model(**inputs).logits.squeeze(-1)\n",
    "            # Store rewards as a list of PyTorch tensors.\n",
    "            rewards = [r for r in reward_logits]\n",
    "\n",
    "            # Log the mean reward for this batch \n",
    "            # This is the key metric to watch.\n",
    "            batch_mean_reward = torch.tensor(rewards).mean().item()\n",
    "            mean_rewards.append(batch_mean_reward)\n",
    "            # Print the mean reward every 10 steps to monitor progress\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Step {i}, Mean Reward: {batch_mean_reward:.4f}\")\n",
    "            \n",
    "        # D. Perform the PPO optimization step.\n",
    "        # This updates the policy model's weights based on the rewards, while\n",
    "        # also calculating the KL penalty against the reference model.\n",
    "        #stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        # Use the 'queries_list' you already created for the generate step\n",
    "        stats = ppo_trainer.step(queries_list, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "print(\"PPO Training Finished!\")\n",
    "\n",
    "# Save the final tuned model.\n",
    "ppo_trainer.save_pretrained(MODEL_FILE_NAME)\n",
    "print(\"Model saved to 'my_ppo_tuned_model'\")\n",
    "\n",
    "\n",
    "# Plot the reward to observe the mean trend development\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(mean_rewards)\n",
    "# plt.title(\"Mean Reward per Batch\")\n",
    "# plt.xlabel(\"PPO Step\")\n",
    "# plt.ylabel(\"Mean Reward\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
