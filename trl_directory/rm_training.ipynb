{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af51de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig\n",
    ")\n",
    "# Set Log Verbosity\n",
    "# Reduce the log output from the transformers library to keep the console clean.\n",
    "transformers.logging.set_verbosity_warning()\n",
    "\n",
    "\n",
    "# Disable Tokenizer Parallelism \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Configure Hardware Device \n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f355bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core reward computation functions\n",
    "from ipynb.fs.full.reward_computation import (\n",
    "    compute_reward,\n",
    "    rule_compliance_score,\n",
    "    calculate_grammar_score,\n",
    "    calculate_semantic_similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the model initialization to move the model to the MPS device right after it's loaded.\n",
    "def model_init():\n",
    "    model_name = \"distilbert-base-german-cased\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=1, problem_type=\"regression\"\n",
    "    )\n",
    "    return model.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a779eb3",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b003ec",
   "metadata": {},
   "source": [
    "\n",
    "### How Optuna Smartly Finds Hyperparameters\n",
    "\n",
    "Optuna doesn't use a simple grid or random search. By default, it uses a smart Bayesian optimization algorithm called **TPE (Tree-structured Parzen Estimator)** that learns from past results to make better guesses over time.\n",
    "\n",
    "#### The TPE Process\n",
    "1.  **Explore:** It starts with a few random trials to gather initial data.\n",
    "2.  **Model:** It divides the results into a \"good\" group (e.g., top 25% of scores) and a \"bad\" group.\n",
    "3.  **Suggest:** For the next trial, it suggests a new set of hyperparameters that are statistically more likely to be in the \"good\" group than the \"bad\" group.\n",
    "\n",
    "#### Finding the Best Setting\n",
    "* **Grid Search:** Checks every single possible setting. Exhaustive but very slow.\n",
    "* **Random Search:** Tries random settings. Better than grid search, but doesn't learn from its mistakes.\n",
    "* **Optuna (TPE):** Plays \"Hot and Cold.\" It uses the feedback from previous trials (\"warmer\" or \"colder\") to make its next guess much more intelligent, focusing its search on the most promising areas of the hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  HYPERPARAMETER SEARCH CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Define the number of different hyperparameter combinations for Optuna to try.\n",
    "N_TRIALS = 5\n",
    "\n",
    "# Define the search space for each hyperparameter in a centralized dictionary.\n",
    "HP_SEARCH_SPACE = {\n",
    "    \"learning_rate\": {\n",
    "        \"type\": \"float\",\n",
    "        \"low\": 1e-5,\n",
    "        \"high\": 5e-5,\n",
    "        \"log\": True,\n",
    "        # Definition: Controls how much the model's weights are updated during training.\n",
    "        # Importance: It's the most critical hyperparameter. Too high, and the model\n",
    "        # won't converge; too low, and training will be extremely slow. A log scale\n",
    "        # is used because the optimal value can vary by orders of magnitude.\n",
    "    },\n",
    "    \"num_train_epochs\": {\n",
    "        \"type\": \"int\",\n",
    "        \"low\": 6,\n",
    "        \"high\": 15,\n",
    "        # Definition: The total number of times the training algorithm will pass\n",
    "        # through the entire training dataset.\n",
    "        # Importance: Balances underfitting and overfitting. Too few epochs,\n",
    "        # and the model won't learn enough; too many, and it may memorize the\n",
    "        # training data and perform poorly on new data.\n",
    "    },\n",
    "    \"per_device_train_batch_size\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"choices\": [32, 64],\n",
    "        # Definition: The number of training examples used in a single forward/backward pass.\n",
    "        # Importance: Affects training stability and memory usage. Larger\n",
    "        # batches provide more stable gradient estimates but require more GPU RAM.\n",
    "    },\n",
    "    \"weight_decay\": {\n",
    "        \"type\": \"float\",\n",
    "        \"low\": 0.0,\n",
    "        \"high\": 0.1,\n",
    "        # Definition: A regularization technique that penalizes large weights in the model.\n",
    "        # Importance: Helps prevent overfitting by keeping the model's weights small\n",
    "        # and simple, improving its ability to generalize to new data.\n",
    "    },\n",
    "    \"lr_scheduler_type\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"choices\": [\"linear\", \"cosine\"],\n",
    "        # What it is: The strategy for changing the learning rate during training.\n",
    "        # Importance: A good schedule (like a linear decay) helps the model\n",
    "        # converge faster and more reliably than a constant learning rate.\n",
    "    },\n",
    "    \"warmup_ratio\": {\n",
    "        \"type\": \"float\",\n",
    "        \"low\": 0.0,\n",
    "        \"high\": 0.1,\n",
    "        # What it is: The fraction of total training steps used for a \"warmup\" phase.\n",
    "        # Importance: The learning rate starts at 0 and slowly increases to its\n",
    "        # target value. This prevents large, unstable updates at the beginning of\n",
    "        # training, allowing the model to stabilize first.\n",
    "    },\n",
    "    \"gradient_accumulation_steps\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"choices\": [1, 2],\n",
    "        # What it is: The number of smaller batches to process before performing a\n",
    "        # single model weight update.\n",
    "        # Importance: It's a trick to simulate a larger batch size without\n",
    "        # using more memory. An effective batch size of 32 (16 * 2) can lead\n",
    "        # to more stable training.\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a718bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_hp_space(trial):\n",
    "    \"\"\"\n",
    "    Dynamically creates the hyperparameter search space for an Optuna trial by\n",
    "    reading from the global HP_SEARCH_SPACE configuration dictionary.\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    # Loop through each parameter defined in the configuration.\n",
    "    for name, config in HP_SEARCH_SPACE.items():\n",
    "        param_type = config[\"type\"]\n",
    "        \n",
    "        # Call the appropriate Optuna `suggest` method based on the parameter's type.\n",
    "        if param_type == \"float\":\n",
    "            params[name] = trial.suggest_float(\n",
    "                name, config[\"low\"], config[\"high\"], log=config.get(\"log\", False)\n",
    "            )\n",
    "        elif param_type == \"int\":\n",
    "            params[name] = trial.suggest_int(\n",
    "                name, config[\"low\"], config[\"high\"]\n",
    "            )\n",
    "        elif param_type == \"categorical\":\n",
    "            params[name] = trial.suggest_categorical(\n",
    "                name, config[\"choices\"]\n",
    "            )\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f740360",
   "metadata": {},
   "source": [
    "## CANDIDATE_WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2800e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_WEIGHTS = [\n",
    "    {\"name\": \"balanced\", \"weights\": {\"rules_score\": 0.5, \"meaning_score\": 0.25, \"grammar_score\": 0.25}}, #baseline\n",
    "    {\"name\": \"rules_heavy\", \"weights\": {\"rules_score\": 0.7, \"meaning_score\": 0.2, \"grammar_score\": 0.1}},\n",
    "    {\"name\": \"meaning_heavy\", \"weights\": {\"rules_score\": 0.2, \"meaning_score\": 0.7, \"grammar_score\": 0.1}},\n",
    "    {\"name\": \"grammar_focused\", \"weights\": {\"rules_score\": 0.2, \"meaning_score\": 0.1, \"grammar_score\": 0.7}},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45928690",
   "metadata": {},
   "source": [
    "## Data Load & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b5892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefine the preprocessing function\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    \"\"\"Tokenizes the 'simplified' text column.\"\"\"\n",
    "    # Important: Tokenization is applied on the 'simplified' column now, which becomes the 'text' for the RM\n",
    "    return tokenizer(examples[\"simplified\"], truncation=True, padding=\"max_length\", max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61a0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load, Split, and Tokenize Data ONCE ---\n",
    "print(\"--- Loading and tokenizing data once ---\")\n",
    "df = pd.read_csv(\"data/ordered_simplifications_with_rules.csv\", index_col=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ecd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795251d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"original_sentence\": \"original\", \"final_simplification\": \"simplified\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7169eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use for Smaller Dataset\n",
    "#df = df.sample(n=50, random_state=42) #reduce size for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the initial dataset from the full DataFrame\n",
    "full_dataset = Dataset.from_pandas(df)\n",
    "# Split it into train and test sets\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.15, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0d249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and apply tokenization on both split sets\n",
    "tokenizer_rm = AutoTokenizer.from_pretrained(\"distilbert-base-german-cased\")\n",
    "\n",
    "# Tokenize the base train and test sets without the labels\n",
    "# We keep the original text columns to calculate rewards later\n",
    "tokenized_train_base = split_dataset[\"train\"].map(\n",
    "    lambda examples: preprocess_function(examples, tokenizer_rm),\n",
    "    batched=True\n",
    ")\n",
    "tokenized_test_base = split_dataset[\"test\"].map(\n",
    "    lambda examples: preprocess_function(examples, tokenizer_rm),\n",
    "    batched=True\n",
    ")\n",
    "print(\"--- Data tokenized successfully. Starting grid search... ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa65a15",
   "metadata": {},
   "source": [
    "## Eval Custom Metrics for regression RM\n",
    "- for the chosen regression RM model MSE loss is chosen\n",
    "- the following metrics are also loggeed\n",
    "  - MSE (Mean Squared Error) → matches your training loss, so you can track consistency.\n",
    "  - MAE (Mean Absolute Error) → more interpretable (average absolute difference).\n",
    "  - R² (Coefficient of Determination) → tells you how well your model explains variance (1 = perfect, 0 = baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f9130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.squeeze()   # shape: (batch,)\n",
    "    labels = labels.squeeze()\n",
    "\n",
    "    mse = mean_squared_error(labels, preds)\n",
    "    mae = mean_absolute_error(labels, preds)\n",
    "    r2  = r2_score(labels, preds)\n",
    "\n",
    "    return {\"mse\": mse, \"mae\": mae, \"r2\": r2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35283f93",
   "metadata": {},
   "source": [
    "## Redirecting Standard Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing a fix to remove unnecessary output originating from compound-splitter library\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout_stderr():\n",
    "    \"\"\"A context manager that redirects stdout and stderr to devnull\"\"\"\n",
    "    # This works on Mac/Linux to silence all output, including from underlying libraries\n",
    "    with open(os.devnull, 'w') as fnull:\n",
    "        old_stdout, old_stderr = sys.stdout, sys.stderr\n",
    "        sys.stdout, sys.stderr = fnull, fnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout, sys.stderr = old_stdout, old_stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b419b309",
   "metadata": {},
   "source": [
    "## Helper Functions for Reward Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13856ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  HELPER FUNCTIONS FOR REWARD CALCULATION\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_stable_parts_batched(examples):\n",
    "    \"\"\"\n",
    "    Calculates the reward components that are stable during multiprocessing.\n",
    "    \n",
    "    This function processes a BATCH of text examples for high efficiency. It's\n",
    "    specifically designed to be the target function for `datasets.map()` when\n",
    "    running in parallel. It handles the spaCy and language_tool calculations.\n",
    "    \n",
    "    Args:\n",
    "        examples (dict): A dictionary from `datasets.map` where each key\n",
    "                         (e.g., 'simplified') maps to a list of values.\n",
    "                         \n",
    "    Returns:\n",
    "        dict: A dictionary containing lists of the calculated scores, which\n",
    "              will be added as new columns to the dataset.\n",
    "    \"\"\"\n",
    "    # Extract the list of sentences for this batch.\n",
    "    simplified_batch = examples['simplified']\n",
    "    \n",
    "    # Use spaCy's nlp.pipe() for highly optimized, batched NLP processing.\n",
    "    # Note: nlp.pipe() returns a generator. Since generators are consumed after one\n",
    "    # iteration, two separate ones need to be created for the two list comprehensions below.\n",
    "    docs_for_rules = nlp.pipe(simplified_batch)\n",
    "    docs_for_grammar = nlp.pipe(simplified_batch)\n",
    "    \n",
    "    # Calculate the scores for each document in the batch.\n",
    "    rules_scores = [rule_compliance_score(doc) for doc in docs_for_rules]\n",
    "    grammar_scores = [calculate_grammar_score(doc.text) for doc in docs_for_grammar]\n",
    "\n",
    "    # The returned dictionary's keys will become the new column names in the dataset.\n",
    "    return {\"rules_score\": rules_scores, \"grammar_score\": grammar_scores}\n",
    "\n",
    "\n",
    "def calculate_rewards_for_split(dataset_split, weights, num_proc, desc_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Orchestrates the full two-stage reward calculation for a given dataset split.\n",
    "    \n",
    "    This function encapsulates the entire complex reward logic:\n",
    "    - Stage 1: Computes stable scores (rules, grammar) in parallel for speed.\n",
    "    - Stage 2: Computes the SBERT meaning score sequentially to avoid crashes.\n",
    "    - Stage 3: Combines the scores from both stages using the provided weights.\n",
    "    \n",
    "    Args:\n",
    "        dataset_split (Dataset): The dataset slice (e.g., train or test) to process.\n",
    "        weights (dict): The dictionary of weights for the current grid search configuration.\n",
    "        num_proc (int): The number of CPU cores to use for Stage 1.\n",
    "        desc_prefix (str): A string (like \"train\" or \"test\") to label the progress bars.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of the final, weighted reward scores for the dataset split.\n",
    "    \"\"\"\n",
    "    # --- STAGE 1: Parallel calculation of stable scores ---\n",
    "    # silence all stdout/stderr from subprocesses to prevent the noisy\n",
    "    # `german-compound-splitter` library from creating an I/O bottleneck.\n",
    "    with suppress_stdout_stderr():\n",
    "        stable_scores_ds = dataset_split.map(\n",
    "            compute_stable_parts_batched,\n",
    "            batched=True,\n",
    "            batch_size=100,\n",
    "            num_proc=num_proc,\n",
    "            desc=f\"Calculating stable scores ({desc_prefix})\"\n",
    "        )\n",
    "\n",
    "    # --- STAGE 2: Sequential calculation of the SBERT meaning score ---\n",
    "    # This part is run on a single core because the SentenceTransformer model\n",
    "    # was found to be incompatible with multiprocessing, causing crashes.\n",
    "    meaning_scores = [\n",
    "        calculate_semantic_similarity(ex['original'], ex['simplified']) \n",
    "        for ex in tqdm(dataset_split, desc=f\"Calculating meaning scores ({desc_prefix})\")\n",
    "    ]\n",
    "    \n",
    "    # --- STAGE 3: Combine all calculated scores ---\n",
    "    # This loop combines the results from the parallel and sequential stages\n",
    "    # into a final, weighted reward score for each example.\n",
    "    final_rewards = []\n",
    "    for i in range(len(dataset_split)):\n",
    "        r_s = stable_scores_ds[i][\"rules_score\"]  # From Stage 1\n",
    "        g_s = stable_scores_ds[i][\"grammar_score\"] # From Stage 1\n",
    "        m_s = meaning_scores[i]                   # From Stage 2\n",
    "        \n",
    "        # Apply the weights for the current experiment configuration.\n",
    "        reward = (weights[\"rules_score\"] * r_s +\n",
    "                  weights[\"grammar_score\"] * g_s +\n",
    "                  weights[\"meaning_score\"] * m_s)\n",
    "        final_rewards.append(reward)\n",
    "        \n",
    "    return final_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae2206",
   "metadata": {},
   "source": [
    "## Helper Functions for Training Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40429de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "#  HELPER FUNCTIONS FOR THE TRAINING PIPELINE\n",
    "# ==============================================================================\n",
    "\n",
    "def prepare_datasets_for_run(tokenized_train_base, tokenized_test_base, train_rewards, test_rewards):\n",
    "    \"\"\"Adds reward labels to the tokenized datasets and sets the final PyTorch format.\"\"\"\n",
    "    # Add the calculated rewards as the 'labels' column for this training run.\n",
    "    train_dataset = tokenized_train_base.add_column(\"labels\", train_rewards)\n",
    "    test_dataset = tokenized_test_base.add_column(\"labels\", test_rewards)\n",
    "    \n",
    "    # Set the dataset format to PyTorch tensors for compatibility with the Trainer.\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def run_hyperparameter_search(trainer, config_name):\n",
    "    \"\"\"Runs the hyperparameter search for a given trainer and returns the best run.\"\"\"\n",
    "    print(f\"--- Running hyperparameter search for {config_name} ---\")\n",
    "    \n",
    "    # This call now uses the N_TRIALS variable defined in your configuration.\n",
    "    best_run = trainer.hyperparameter_search(\n",
    "        direction=\"minimize\",      # We want to minimize the objective (MSE).\n",
    "        hp_space=model_hp_space,   # The function that builds the search space.\n",
    "        n_trials=N_TRIALS          # The number of trials, controlled from the top of the notebook.\n",
    "    )\n",
    "    print(f\"Best run for {config_name}: {best_run}\")\n",
    "    return best_run\n",
    "\n",
    "\n",
    "def train_and_save_final_model(trainer, best_run, output_dir_base, config_name):\n",
    "    \"\"\"Trains the final model using the best hyperparameters and saves it.\"\"\"\n",
    "    print(f\"--- Training final model for {config_name} with best hyperparameters ---\")\n",
    "    \n",
    "    # Apply the best parameters found during the search to the trainer's arguments.\n",
    "    for k, v in best_run.hyperparameters.items():\n",
    "        setattr(trainer.args, k, v)\n",
    "    \n",
    "    # Start the final training run.\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the final, optimized model to a unique directory.\n",
    "    final_output_dir = f\"{output_dir_base}_final\"\n",
    "    trainer.save_model(final_output_dir)\n",
    "    print(f\"--- Saved final optimized model to {final_output_dir} ---\\n\")\n",
    "\n",
    "\"\"\"\n",
    "Reusable Summary Function:  It takes a single result dictionary and prints it in a clean format.\n",
    "\"\"\"\n",
    "def print_run_summary(result):\n",
    "    \"\"\"Prints a formatted summary for a single training run.\"\"\"\n",
    "    print(f\"\\n--- Summary for Configuration: {result['config_name']} ---\")\n",
    "    print(\"  Best Hyperparameters Found:\")\n",
    "    for param, value in result['best_hyperparameters'].items():\n",
    "        print(f\"    - {param}: {value}\")\n",
    "    \n",
    "    print(\"  Final Evaluation Metrics:\")\n",
    "    for metric, value in result['final_metrics'].items():\n",
    "        # We only print metrics that start with 'eval_' and format them nicely\n",
    "        if 'eval_' in metric:\n",
    "            print(f\"    - {metric}: {value:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e679a",
   "metadata": {},
   "source": [
    "## Trainig Loop with Grid Search for all 4 CANDIDATE_WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  MAIN GRID SEARCH AND TRAINING LOOP \n",
    "# ==============================================================================\n",
    "\n",
    "# --- Initialize a list to store results ---\n",
    "all_results = []\n",
    "\n",
    "# This main loop iterates through each set of candidate weights, calculates the\n",
    "# corresponding rewards, and then (in subsequent code) trains a reward model.\n",
    "for config in CANDIDATE_WEIGHTS:\n",
    "    # --- SETUP FOR THE CURRENT LOOP ITERATION ---\n",
    "    config_name = config[\"name\"]\n",
    "    weights = config[\"weights\"]\n",
    "    output_dir_base = f\"rm_out_{config_name}\"\n",
    "\n",
    "    print(f\"\\n--- Processing configuration: {config_name} ---\")\n",
    "    print(f\"Weights: {weights}\")\n",
    "\n",
    "    # Set the number of CPU cores for parallel processing.\n",
    "    num_cores_to_use = 12\n",
    "\n",
    "    # --- REWARD CALCULATION ---\n",
    "    # Call reusable orchestration function to get the rewards for both the\n",
    "    # training and test sets for this specific weight configuration.\n",
    "    \n",
    "    print(\"--- Calculating rewards for the TRAINING set ---\")\n",
    "    train_rewards = calculate_rewards_for_split(\n",
    "        split_dataset[\"train\"], \n",
    "        weights, \n",
    "        num_cores_to_use, \n",
    "        desc_prefix=\"train\"\n",
    "    )\n",
    "\n",
    "    print(\"--- Calculating rewards for the TEST set ---\")\n",
    "    test_rewards = calculate_rewards_for_split(\n",
    "        split_dataset[\"test\"], \n",
    "        weights, \n",
    "        num_cores_to_use, \n",
    "        desc_prefix=\"test\"\n",
    "    )\n",
    "    \n",
    "    print(f\"--- Finished calculating rewards for {config_name} ---\")\n",
    "\n",
    "\n",
    "    # The 'train_rewards' and 'test_rewards' variables are now ready to be\n",
    "    # used to train the reward model for this specific 'config'.\n",
    "\n",
    "    # ==============================================================================\n",
    "    #  POST-REWARD CALCULATION: DATASET PREP & MODEL TRAINING\n",
    "    # ==============================================================================\n",
    "    # This code block is inside the `for config in CANDIDATE_WEIGHTS:` loop.\n",
    "    # The 'train_rewards' and 'test_rewards' variables are now ready.\n",
    "\n",
    "    # --- b. Dataset Preparation ---\n",
    "    # Call our helper function to create the final datasets for this specific training run.\n",
    "    # This function takes the pre-tokenized text and merges it with the `train_rewards`\n",
    "    # and `test_rewards` lists that were just calculated using the current `weights`.\n",
    "    train_ds, test_ds = prepare_datasets_for_run(\n",
    "        tokenized_train_base, tokenized_test_base, train_rewards, test_rewards\n",
    "    )\n",
    "\n",
    "    # --- c. Trainer Setup ---\n",
    "    # First, we configure the training process using the `TrainingArguments` class.\n",
    "    # This object holds all the settings for how the model will be trained and evaluated.\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir_base,      # Directory to save model checkpoints and outputs.\n",
    "        use_mps_device=True,             # Explicitly leverage the Apple Silicon GPU (Metal Performance Shaders).\n",
    "        evaluation_strategy=\"epoch\",     # Run an evaluation on the test set at the end of each training epoch.\n",
    "        save_strategy=\"epoch\",           # Save a model checkpoint at the end of each epoch.\n",
    "        load_best_model_at_end=True,     # After training, automatically load the checkpoint with the best performance.\n",
    "        metric_for_best_model=\"mse\",     # Use Mean Squared Error (MSE) as the key metric to determine the \"best\" model.\n",
    "        greater_is_better=False,         # Specifies that a lower value for `metric_for_best_model` (MSE) is better.\n",
    "        logging_strategy=\"epoch\",        # How often to log training metrics. \"epoch\" is less noisy than \"steps\".,\n",
    "        dataloader_pin_memory=False,     # Disable the feature and silence the warning since not applicable for Mac with an M-series chip uses a Unified Memory Architecture \n",
    "        save_total_limit=1, # Only keep the single best checkpoint on disk\n",
    "\n",
    "    )\n",
    "\n",
    "    # Next, we initialize the `Trainer` object.\n",
    "    # This is the main orchestrator from the Hugging Face library that brings together\n",
    "    # the model, datasets, and training configuration.\n",
    "    trainer = Trainer(\n",
    "        # `model_init` is a function that returns a fresh, untrained model.\n",
    "        # This is crucial for hyperparameter search to ensure each trial starts from scratch.\n",
    "        model_init=model_init,\n",
    "        args=training_args,                 # The training configuration we just defined.\n",
    "        train_dataset=train_ds,             # The prepared training dataset.\n",
    "        eval_dataset=test_ds,               # The prepared test dataset.\n",
    "        compute_metrics=compute_metrics,    # The function to calculate metrics like MSE, MAE, and R².\n",
    "        tokenizer=tokenizer_rm,             # The tokenizer, used for data collation.\n",
    "    )\n",
    "\n",
    "    # --- d. Hyperparameter Search and Final Training ---\n",
    "    # Now we execute the main logic using our custom helper functions for clarity.\n",
    "    # First, find the best set of hyperparameters for the current reward model configuration.\n",
    "    best_run = run_hyperparameter_search(trainer, config_name)\n",
    "    # Then, use those best hyperparameters to train a final, optimized model.\n",
    "    train_and_save_final_model(trainer, best_run, output_dir_base, config_name)\n",
    "\n",
    "    # --- e. Evaluate the Final Model and Store Results for Summary ---\n",
    "    # After the final model for this configuration has been trained, we run a final\n",
    "    # evaluation on the test set to get its performance metrics.\n",
    "    print(f\"--- Evaluating final model for {config_name} ---\")\n",
    "    final_metrics = trainer.evaluate()\n",
    "\n",
    "    # We append the results of this entire run into a list. This allows us to\n",
    "    # print a full summary of all configurations after the main loop has finished.\n",
    "    current_result = {\n",
    "        \"config_name\": config_name,\n",
    "        \"best_hyperparameters\": best_run.hyperparameters,\n",
    "        \"final_metrics\": final_metrics\n",
    "    }\n",
    "    all_results.append(current_result)\n",
    "    print_run_summary(current_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  FINAL SUMMARY OF ALL RUNS (NOW USING THE HELPER FUNCTION)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\\n=================================================\")\n",
    "print(\"           GRID SEARCH FINAL SUMMARY\")\n",
    "print(\"=================================================\\n\")\n",
    "\n",
    "for result in all_results:\n",
    "    # Call the same helper function to print each result\n",
    "    print_run_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f887b",
   "metadata": {},
   "source": [
    "## Model_rm has been trained with the whole dataset now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
