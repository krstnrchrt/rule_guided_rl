{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06602442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json, ast, re\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from final_eval_calc_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ceda89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.reward_computation import rule_compliance_score, calculate_semantic_similarity, calculate_grammar_score, compute_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7020296",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_OUTPUT_DIR = \"eval_final_results/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c724ab",
   "metadata": {},
   "source": [
    "## Load files and prepare dfs before applying calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0cd8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "df_ex1a = pd.read_csv(\"eval_final_results/1b_ppo_evaluation_result_PPO_model_2K_4E_simple_q_28.csv\")\n",
    "df_ex1b = pd.read_csv(\"eval_final_results/1a_ppo_evaluation_result_PPO_model_2K_4E_28.csv\")\n",
    "\n",
    "df_ex2a = pd.read_csv(\"eval_final_results/2a_ppo_evaluation_result_PPO_model_2K_4E_SFT_KL_0.05.csv\")\n",
    "df_ex2b = pd.read_csv(\"eval_final_results/2b_ppo_evaluation_result_2K_4E_SFT_KL_high.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19780d86",
   "metadata": {},
   "source": [
    "# Overall Analysis - This step calcualtes the defined metrics based on the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc53ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define reward weights #putting more stress on the semantic similarity\n",
    "REWARD_WEIGHTS = {\n",
    "    \"rule_score\": 0.3,\n",
    "    \"semantic_score\": 0.5,\n",
    "    \"grammar_score\": 0.2,\n",
    "}\n",
    "\n",
    "\n",
    "# 2) Define evaluation function\n",
    "def eval_output(df, text_col=\"ppo_response\", weights=REWARD_WEIGHTS):\n",
    "    df_eval = df.copy()\n",
    "\n",
    "    # --- Compute component scores ---\n",
    "    df_eval[\"semantic_score\"] = df_eval.apply(\n",
    "        lambda r: calculate_semantic_similarity(r[\"original\"], r[text_col]), axis=1\n",
    "    )\n",
    "    df_eval[\"grammar_score\"] = df_eval[text_col].apply(calculate_grammar_score)\n",
    "    \n",
    "    df_eval[\"rule_score\"] = df_eval.apply(\n",
    "        lambda r: rule_compliance_score(r[text_col]), axis=1\n",
    "    )\n",
    "\n",
    "    # --- Weighted total reward (Reward Function) ---\n",
    "    df_eval[\"total_reward\"] = (\n",
    "        df_eval[\"rule_score\"] * weights[\"rule_score\"]\n",
    "        + df_eval[\"semantic_score\"] * weights[\"semantic_score\"]\n",
    "        + df_eval[\"grammar_score\"] * weights[\"grammar_score\"]\n",
    "    )\n",
    "\n",
    "    # Input length\n",
    "    #df_eval[\"input_len_chars\"] = df_eval[\"original\"].astype(str).str.len() # number of characters \n",
    "    df_eval[\"input_len_tokens\"] = df_eval[\"original\"].astype(str).str.split().map(len) # number of whitespace-split tokens\n",
    "\n",
    "    # Output length\n",
    "    #df_eval[\"output_len_chars\"] = df_eval[text_col].astype(str).str.len()\n",
    "    df_eval[\"output_len_tokens\"] = df_eval[text_col].astype(str).str.split().map(len)\n",
    "\n",
    "    # Ratios (output vs input)\n",
    "    #df_eval[\"len_ratio_chars\"] = df_eval[\"output_len_chars\"] / df_eval[\"input_len_chars\"]\n",
    "    df_eval[\"len_ratio_tokens\"] = df_eval[\"output_len_tokens\"] / df_eval[\"input_len_tokens\"]\n",
    "\n",
    "    return df_eval\n",
    "\n",
    "# 3) Load PPO evaluation files into dict\n",
    "dfs = {\n",
    "    \"exp1a\": df_ex1a,\n",
    "    \"exp1b\": df_ex1b,\n",
    "    \"exp2a\": df_ex2a,\n",
    "    \"exp2b\": df_ex2b,\n",
    "}\n",
    "\n",
    "# 4) Evaluate all experiments (PPO only, no rule explode)\n",
    "results = {}\n",
    "for name, df in dfs.items():\n",
    "    df_eval = eval_output(df, text_col=\"ppo_response\")\n",
    "    results[name] = df_eval\n",
    "    df_eval.to_csv(f\"eval_final_results/{name}_per_sentence_scored.csv\", index=False)\n",
    "    print(f\"Finished {name}\")\n",
    "\n",
    "# 5) Build summary (mean + median per metric)\n",
    "records = []\n",
    "for name, df_eval in results.items():\n",
    "    records.append({\n",
    "        \"experiment\": name,\n",
    "        \"mean_semantic\": df_eval[\"semantic_score\"].mean(),\n",
    "        \"median_semantic\": df_eval[\"semantic_score\"].median(),\n",
    "        \"mean_rule\": df_eval[\"rule_score\"].mean(),\n",
    "        \"median_rule\": df_eval[\"rule_score\"].median(),\n",
    "        \"mean_grammar\": df_eval[\"grammar_score\"].mean(),\n",
    "        \"median_grammar\": df_eval[\"grammar_score\"].median(),\n",
    "        \"mean_total_reward\": df_eval[\"total_reward\"].mean(),\n",
    "        \"median_total_reward\": df_eval[\"total_reward\"].median(),\n",
    "        \"len_ratio_tokens\": df_eval[\"len_ratio_tokens\"].median()\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(records)\n",
    "summary_df.to_csv(\"eval_final_results/ppo_combined_summary.csv\", index=False)\n",
    "\n",
    "print(summary_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f1d3c",
   "metadata": {},
   "source": [
    "## Load the generated log to continue analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa279d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "df_ex1a_prep = pd.read_csv(\"eval_final_results/exp1a_per_sentence_scored.csv\")\n",
    "df_ex1b_prep = pd.read_csv(\"eval_final_results/exp1b_per_sentence_scored.csv\")\n",
    "\n",
    "df_ex2a_prep = pd.read_csv(\"eval_final_results/exp2a_per_sentence_scored.csv\")\n",
    "df_ex2b_prep = pd.read_csv(\"eval_final_results/exp2b_per_sentence_scored.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263dc49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Load PPO evaluation files into dict\n",
    "dfs = {\n",
    "    \"1A\": df_ex1a_prep,\n",
    "    \"1B\": df_ex1b_prep,\n",
    "    \"2A\": df_ex2a_prep,\n",
    "    \"2B\": df_ex2b_prep,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_scores_summary = []\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    if \"ppo_reward_score\" in df.columns:\n",
    "        ppo_scores_summary.append({\n",
    "            \"experiment\": name,\n",
    "            \"mean_ppo_score\": df[\"ppo_reward_score\"].mean(),\n",
    "            \"median_ppo_score\": df[\"ppo_reward_score\"].median(),\n",
    "            \"min_ppo_score\": df[\"ppo_reward_score\"].min(),\n",
    "            \"max_ppo_score\": df[\"ppo_reward_score\"].max(),\n",
    "        })\n",
    "    else:\n",
    "        print(f\"⚠️ {name} has no 'ppo_reward_score' column\")\n",
    "\n",
    "ppo_summary_df = pd.DataFrame(ppo_scores_summary)\n",
    "print(ppo_summary_df)\n",
    "\n",
    "# Save to CSV if needed\n",
    "ppo_summary_df.to_csv(\"eval_final_results/ppo_reward_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7faa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outputs(df, text_col=\"ppo_response\"):\n",
    "    \"\"\"\n",
    "    Run structural sanity checks on PPO outputs.\n",
    "    1) Check if output ends with proper punctuation.\n",
    "    2) Check if output contains unwanted metadata (e.g., Quelle, Source, http).\n",
    "    \"\"\"\n",
    "    valid_endings = (\".\", \"!\", \"?\")\n",
    "    unwanted_patterns = re.compile(r\"(quelle|source|http[s]?://)\", re.IGNORECASE)\n",
    "\n",
    "    # Check sentence endings\n",
    "    df[\"ends_with_punct\"] = df[text_col].astype(str).str.strip().str.endswith(valid_endings)\n",
    "\n",
    "    # Check unwanted patterns\n",
    "    df[\"contains_metadata\"] = df[text_col].astype(str).str.contains(unwanted_patterns, regex=True)\n",
    "\n",
    "    # Summary stats\n",
    "    stats = {\n",
    "        \"total_outputs\": len(df),\n",
    "        \"valid_endings\": df[\"ends_with_punct\"].sum(),\n",
    "        \"invalid_endings\": (~df[\"ends_with_punct\"]).sum(),\n",
    "        \"percent_valid_endings\": df[\"ends_with_punct\"].mean() * 100,\n",
    "        \"contains_metadata\": df[\"contains_metadata\"].sum(),\n",
    "        \"percent_metadata\": df[\"contains_metadata\"].mean() * 100,\n",
    "    }\n",
    "\n",
    "    return stats, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20459f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for name, df in dfs.items():\n",
    "    stats, df_checked = check_outputs(df, text_col=\"ppo_response\")\n",
    "    results[name] = stats\n",
    "    print(f\"\\n{name} results:\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "# Optionally create a summary DataFrame\n",
    "summary_df = pd.DataFrame(results).T\n",
    "summary_df.to_csv(\"eval_final_results/ppo_output_sanity_checks.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d8ba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Semantic vs Rule Adherence for all PPO outputs\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (name, df) in zip(axes, dfs.items()):\n",
    "    ax.scatter(df[\"semantic_score\"], df[\"rule_score\"], alpha=0.5, s=15)\n",
    "    ax.set_title(f\"{name}: Semantic Score vs Rule Adherence\")\n",
    "    ax.set_xlabel(\"Semantic Score\")\n",
    "    ax.set_ylabel(\"Rule Score\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "plt.suptitle(\"Trade-off Between Semantic Preservation and Rule Adherence Across PPO Variants\", fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.savefig('semanticvsrule.pdf')\n",
    "plt.savefig('semanticvsrule.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071a743",
   "metadata": {},
   "source": [
    "# This section calcualtes a rule-specific deep-dive into the metrics performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa27954",
   "metadata": {},
   "source": [
    "## Aggregated Rule Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_any(obj):\n",
    "    \"\"\"\n",
    "    Recursively parse/flatten lists and stringified lists into a Python list of strings.\n",
    "    \"\"\"\n",
    "    # None / NaN\n",
    "    if obj is None or (isinstance(obj, float) and pd.isna(obj)):\n",
    "        return []\n",
    "\n",
    "    # Already a list -> flatten each element\n",
    "    if isinstance(obj, list):\n",
    "        out = []\n",
    "        for el in obj:\n",
    "            out.extend(_parse_any(el))\n",
    "        return out\n",
    "\n",
    "    # Strings: may be raw rule, JSON, python literal, or a quoted string containing a list\n",
    "    if isinstance(obj, str):\n",
    "        s = obj.strip()\n",
    "        if s in (\"\", \"[]\"):\n",
    "            return []\n",
    "\n",
    "        # If it looks like a quoted list string:  '\"[...]\"' or '\\'[...]\\''\n",
    "        if len(s) >= 4 and s[0] in (\"'\", '\"') and s[-1] == s[0] and s[1] == '[' and s[-2] == ']':\n",
    "            s = s[1:-1].strip()  # unwrap quotes around the whole [ ... ]\n",
    "\n",
    "        # Try JSON\n",
    "        try:\n",
    "            val = json.loads(s)\n",
    "            # If val is list or nested, recurse\n",
    "            if isinstance(val, (list, str)):\n",
    "                return _parse_any(val)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Try python literal (handles single quotes)\n",
    "        try:\n",
    "            val = ast.literal_eval(s)\n",
    "            if isinstance(val, (list, str)):\n",
    "                return _parse_any(val)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # As a last parsing attempt, handle bracketed single item: [\"rule\"]\n",
    "        m = re.match(r'^\\[\\s*[\\'\"]?([A-Za-z0-9_]+)[\\'\"]?\\s*\\]$', s)\n",
    "        if m:\n",
    "            return [m.group(1)]\n",
    "\n",
    "        # Comma-separated fallback (no brackets)\n",
    "        if \",\" in s and \"[\" not in s and \"]\" not in s:\n",
    "            parts = [p.strip().strip(\"'\\\"\") for p in s.split(\",\") if p.strip()]\n",
    "            return parts\n",
    "\n",
    "        # Otherwise treat as atomic rule token\n",
    "        return [s.strip(\"'\\\"\")]\n",
    "\n",
    "    # Anything else -> string\n",
    "    return [str(obj).strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf9a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df_in, verbose=False):\n",
    "        # --- Work on a copy ---\n",
    "    df = df_in.copy()\n",
    "\n",
    "    # Parse applied_rules\n",
    "    df[\"applied_rules_list\"] = df[\"applied_rules\"].apply(_parse_any)\n",
    "\n",
    "    # Keep only allowed rules & dedup\n",
    "    df[\"applied_rules_list\"] = df[\"applied_rules_list\"].apply(\n",
    "        lambda L: dedup_preserve_order([r for r in L if r in ALLOWED_RULES])\n",
    "    )\n",
    "\n",
    "    # Explode\n",
    "    df_out = (\n",
    "        df.explode(\"applied_rules_list\")\n",
    "          .rename(columns={\"applied_rules_list\": \"rule\"})\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "     # Normalize unwanted rule strings\n",
    "    df_out[\"rule\"] = df_out[\"rule\"].apply(normalize_rule_string)\n",
    "    \n",
    "\n",
    "\n",
    "    # Drop empty rules\n",
    "    df_out = df_out[~df_out[\"rule\"].isna() & (df_out[\"rule\"].astype(str).str.strip() != \"\")]\n",
    "    # Exclude punctuation category\n",
    "    df_out = df_out[df_out[\"rule\"] != \"clean_punctuation\"]\n",
    "   \n",
    "\n",
    "    # Verbose output\n",
    "    if verbose:\n",
    "        print(\"Parsed sample:\", df[\"applied_rules_list\"].head(10).tolist())\n",
    "        print(\"Original rows:\", len(df))\n",
    "        print(\"Exploded rows:\", len(df_out))\n",
    "        print(\"\\nCounts per rule:\")\n",
    "        print(df_out[\"rule\"].value_counts())\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_ex1a_prep, df_ex1b_prep, df_ex2a_prep, df_ex2b_prep]\n",
    "\n",
    "processed = [preprocess_df(df, verbose=False) for df in dfs]\n",
    "df1a_prepared, df1b_prepared, df2a_prepared, df2b_prepared = processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a06b9",
   "metadata": {},
   "source": [
    "## Apply Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b2271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define reward weights #putting more stress on the semantic similarity\n",
    "REWARD_WEIGHTS = {\n",
    "    \"rule_score\": 0.3,\n",
    "    \"semantic_score\": 0.5,\n",
    "    \"grammar_score\": 0.2,\n",
    "}\n",
    "def eval_output_aggr(df, text_col, weights=REWARD_WEIGHTS):\n",
    "\n",
    "    df_eval = df.copy()\n",
    "# 3) Compute rewards per sentence again\n",
    "    df_eval[\"semantic_score\"] = df_eval.apply(\n",
    "        lambda r: calculate_semantic_similarity(r[\"original\"], r[text_col]), axis=1\n",
    "    )\n",
    "    df_eval[\"grammar_score\"] = df_eval[text_col].apply(calculate_grammar_score)\n",
    "    \n",
    "    df_eval[\"rule_score\"] = df_eval.apply(\n",
    "        lambda r: rule_compliance_score(r[text_col]), axis=1\n",
    "    )\n",
    "\n",
    "    df_eval[\"total_reward\"] = ( df_eval[\"rule_score\"] * REWARD_WEIGHTS[\"rule_score\"]\n",
    "        + df_eval[\"semantic_score\"] * REWARD_WEIGHTS[\"semantic_score\"]\n",
    "        + df_eval[\"grammar_score\"] * REWARD_WEIGHTS[\"grammar_score\"]\n",
    ")\n",
    "    # Input length\n",
    "    df_eval[\"input_len_chars\"] = df_eval[\"original\"].astype(str).str.len()\n",
    "    df_eval[\"input_len_tokens\"] = df_eval[\"original\"].astype(str).str.split().map(len)\n",
    "    # --- Output length \n",
    "    df_eval[\"output_len_chars\"] = df_eval[text_col].astype(str).str.len() #number of characters\n",
    "    df_eval[\"output_len_tokens\"] = df_eval[text_col].astype(str).str.split().map(len) #number of white split tokens\n",
    "    # Ratios (output vs input)\n",
    "    df_eval[\"len_ratio_chars\"] = df_eval[\"output_len_chars\"] / df_eval[\"input_len_chars\"]\n",
    "    df_eval[\"len_ratio_tokens\"] = df_eval[\"output_len_tokens\"] / df_eval[\"input_len_tokens\"]    \n",
    "\n",
    "    # --- Overall summary\n",
    "    summary = {\n",
    "        \"mean_total_reward\": df_eval[\"total_reward\"].mean(), #total reward already computed\n",
    "        \"mean_semantic\": df_eval[\"semantic_score\"].mean(),\n",
    "        \"mean_grammar\": df_eval[\"grammar_score\"].mean(),\n",
    "        \"mean_rule\": df_eval[\"rule_score\"].mean(),\n",
    "        \"mean_len_chars\": df_eval[\"output_len_chars\"].mean(),\n",
    "        \"mean_len_tokens\": df_eval[\"output_len_tokens\"].mean(),\n",
    "    }\n",
    "\n",
    "    return df_eval, summary\n",
    "\n",
    "def rule_aggregated_output(df, text_col):\n",
    "    agg_rule = (\n",
    "        df.groupby(\"rule\")[[\"semantic_score\",\"grammar_score\",\"rule_score\",\"total_reward\"]]\n",
    "        .mean()\n",
    "        .sort_values(\"total_reward\", ascending=False)\n",
    "    )\n",
    "    return agg_rule.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ce83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bundle variables\n",
    "dfs_bundle = {\n",
    "    \"df1a\": df1a_prepared,\n",
    "    \"df1b\": df1b_prepared,\n",
    "    \"df2a\": df2a_prepared,\n",
    "    \"df2b\": df2b_prepared,\n",
    "}\n",
    "\n",
    "text_cols = [\"ppo_response\", \"simplified\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply calculation on all files & save output\n",
    "all_results = {}\n",
    "\n",
    "for name, df in dfs_bundle.items():\n",
    "    for col in text_cols:\n",
    "        tag = f\"{name}_{col}\"   # e.g., df1a_ppo_response, df1a_simplified\n",
    "        \n",
    "        # --- Sentence-level eval ---\n",
    "        df_eval, summary = eval_output_aggr(df.copy(), text_col=col)\n",
    "        \n",
    "        # --- Rule-level aggregates ---\n",
    "        agg = rule_aggregated_output(df_eval, text_col=col)\n",
    "        \n",
    "        # --- Save to dict ---\n",
    "        all_results[tag] = {\n",
    "            \"per_sentence\": df_eval,\n",
    "            \"summary\": summary,\n",
    "            \"per_rule\": agg\n",
    "        }\n",
    "        \n",
    "        # --- Save to disk ---\n",
    "        df_eval.to_csv(f\"eval_final_results/aggregated/{tag}_per_sentence.csv\", index=False)\n",
    "        agg.to_csv(f\"eval_final_results/aggregated/{tag}_per_rule.csv\", index=False)\n",
    "\n",
    "        print(f\"Finished {tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b80883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10df1b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd9843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
