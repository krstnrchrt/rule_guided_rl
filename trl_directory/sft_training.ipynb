{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03bea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1. IMPORTS & INITIAL SETUP\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# The base language model to be fine-tuned.\n",
    "BASE_MODEL_ID = \"frhew/sigdial_ft_a2\"\n",
    "# The directory where the final LoRA adapters will be saved.\n",
    "SFT_MODEL_OUTPUT_DIR = \"my_sft_tuned_model_v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443673c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  2. LOAD AND PREPARE THE DATASET\n",
    "\n",
    "# --- Configuration for saved datasets ---\n",
    "MASTER_TRAIN_DATASET = \"sft_train_dataset.csv\"\n",
    "MASTER_EVAL_DATASET = \"sft_eval_dataset.csv\"\n",
    "MASTER_TEST_DATASET = \"sft_test_dataset.csv\"\n",
    "\n",
    "# --- Load and Clean Data ---\n",
    "df = pd.read_csv(\"data/ordered_simplifications_with_rules_clean.csv\", index_col=0)\n",
    "df.rename(columns={\"original_sentence\": \"original\", \"final_simplification\": \"simplified\"}, inplace=True)\n",
    "faulty_indices = df[\n",
    "    df[\"applied_rules\"].str.contains(\"convert_word_to_number\") &\n",
    "    df[\"simplified\"].str.match(r\"^\\d+$\")\n",
    "].index\n",
    "df = df.drop(faulty_indices)\n",
    "print(f\"Loaded and cleaned {len(df)} rows of data.\")\n",
    "\n",
    "# --- Create Train/Validation/Test Split (80:10:10) ---\n",
    "# First, convert the entire DataFrame into a Hugging Face Dataset object.\n",
    "full_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Step 1: Split the data into a training set (80%) and a temporary set (20%).\n",
    "train_and_temp_split = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_and_temp_split['train']\n",
    "temp_dataset = train_and_temp_split['test'] # This is the 20%\n",
    "\n",
    "# Step 2: Split the temporary set in half to get validation (10%) and test (10%).\n",
    "eval_and_test_split = temp_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "eval_dataset = eval_and_test_split['train']\n",
    "test_dataset = eval_and_test_split['test']\n",
    "\n",
    "print(f\"\\nData split into {len(train_dataset)} training, {len(eval_dataset)} validation, and {len(test_dataset)} test examples.\")\n",
    "\n",
    "# --- Format for SFT ---\n",
    "def format_sft_example(example):\n",
    "    \"\"\"Formats an example with the instruction template for the SFTTrainer.\"\"\"\n",
    "    return {\n",
    "        \"text\": f\"Aufgabe: Vereinfache den folgenden Satz nach Leichter-Sprache-Regeln.\\nSatz: {example['original']}\\nAntwort: {example['simplified']}\"\n",
    "    }\n",
    "\n",
    "# Apply the formatting function to all three splits.\n",
    "train_dataset = train_dataset.map(format_sft_example)\n",
    "eval_dataset = eval_dataset.map(format_sft_example)\n",
    "test_dataset = test_dataset.map(format_sft_example)\n",
    "\n",
    "# --- Save all Three Datasets to Disk for Later Use ---\n",
    "train_dataset.save_to_disk(MASTER_TRAIN_DATASET)\n",
    "eval_dataset.save_to_disk(MASTER_EVAL_DATASET)\n",
    "test_dataset.save_to_disk(MASTER_TEST_DATASET)\n",
    "print(\"\\nTraining, validation, and test datasets have been saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4071f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "#  3. CONFIGURE MODEL, TOKENIZER, AND LORA\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, padding_side='left')\n",
    "# Set the padding token to be the end-of-sequence token if it's not already defined.\n",
    "# This is a standard practice for decoder-only models like Llama.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- PEFT Configuration (LoRA) ---\n",
    "# This configuration tells the trainer to use LoRA for efficient fine-tuning.\n",
    "# It freezes the base model and only trains the small adapter layers.\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Target the attention layers for LoRA adaptation.\n",
    "    # Note: These module names can vary between models. \"q_proj\" and \"v_proj\" are common for Llama.\n",
    "    target_modules=[\"q_proj\", \"v_proj\"] \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5decb54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  4. CONFIGURE AND RUN THE SFTTRAINER\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Load the Base Model with the Correct Precision ---\n",
    "# We load the model here first, specifying bfloat16 for memory and performance gainsfor a Mac.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# --- Define Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=SFT_MODEL_OUTPUT_DIR,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=1,\n",
    "    # Set the strategies to 'steps'\n",
    "    logging_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    # Define the interval 'N'\n",
    "    logging_steps=100,  # Print training loss every 100 steps\n",
    "    eval_steps=100,     # Run evaluation and print validation loss every 100 steps\n",
    "    # Also align saving strategy if you want to save the best model based on steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\", # Use eval_loss to find the best model\n",
    "    save_total_limit=2, # Optional: only keep the best 2 checkpoints\n",
    ")\n",
    "\n",
    "# --- Initialize the SFTTrainer ---\n",
    "# Now, pass the pre-loaded model object instead of the model ID string.\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # <-- Pass the loaded model object here\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset, \n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,\n",
    "    max_seq_length=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Start Training ---\n",
    "print(\"\\n--- Starting Supervised Fine-Tuning (SFT)... ---\")\n",
    "trainer.train()\n",
    "print(\"--- SFT Training Finished. ---\")\n",
    "\n",
    "# --- Save the Final Model ---\n",
    "# This saves the trained LoRA adapter weights to the specified directory.\n",
    "trainer.model.save_pretrained(SFT_MODEL_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(SFT_MODEL_OUTPUT_DIR)\n",
    "print(f\"\\nSFT model adapters and tokenizer saved to: {SFT_MODEL_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c080404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  5. PLOT TRAINING & VALIDATION LOSS CURVE\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Extract the training history ---\n",
    "log_history = trainer.state.log_history\n",
    "log_df = pd.DataFrame(log_history)\n",
    "\n",
    "train_logs = log_df[log_df.get('loss').notna()]\n",
    "eval_logs = log_df[log_df.get('eval_loss').notna()]\n",
    "\n",
    "# --- Create the Plot ---\n",
    "if not train_logs.empty and not eval_logs.empty:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax.plot(train_logs['step'], train_logs['loss'], marker='o', linestyle='-', markersize=4, label='Training Loss')\n",
    "    ax.plot(eval_logs['step'], eval_logs['eval_loss'], marker='s', linestyle='--', markersize=4, label='Validation Loss')\n",
    "\n",
    "    ax.set_title('SFT Training & Validation Loss Curve', fontsize=16)\n",
    "    ax.set_xlabel('Training Steps', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.savefig(\"sft_loss_curve.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nCould not generate plot. Check if training and evaluation logs were created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
