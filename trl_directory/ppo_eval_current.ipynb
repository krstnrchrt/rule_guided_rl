{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6077636",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b874c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVAL_DATA = \"eval_df_23Sep.csv\"\n",
    "EVAL_DATA = \"eval_df_24Sep.csv\"\n",
    "EVAL_DATA_SUBSET = 0.2 # 1 for whole eval_df_23Sep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c168b0",
   "metadata": {},
   "source": [
    "### Step 1: Setup and Loading the Final Model\n",
    "#### The most important step is loading the model correctly. Because you trained with LoRA, you need to first load the original base model and then apply your saved adapter weights on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cb83771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/trl-env/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0925 09:31:02.230000 13304 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# = =============================================================================\n",
    "#  1. IMPORTS & SETUP\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "\n",
    "# --- Configure Hardware Device ---\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c72793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0372984eaf5b43c5bce0b070af18517c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'frhew/sigdial_ft_a2', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded PPO-tuned policy model.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  2. DEFINE PATHS & LOAD COMPONENTS\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Define Paths ---\n",
    "# The original base model your PPO model was fine-tuned from.\n",
    "BASE_MODEL_ID = \"frhew/sigdial_ft_a2\"\n",
    "# The path to your saved PPO model, which contains the LoRA adapter weights.\n",
    "PPO_MODEL_PATH = \"my_ppo_tuned_model_V24.09\"\n",
    "# The path to your custom-trained reward model.\n",
    "RM_PATH = \"rm_out_rules_heavy_final\"\n",
    "\n",
    "# --- Load Base Model ---\n",
    "# First, load the original, pre-trained base model in bfloat16 for memory efficiency.\n",
    "base_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# --- Load the PEFT Model (Policy Model) ---\n",
    "# Now, load the LoRA adapters from your saved directory and apply them to the base model.\n",
    "policy_model = PeftModel.from_pretrained(base_model, PPO_MODEL_PATH)\n",
    "# Merge the adapter weights into the base model and unload the PEFT model.\n",
    "# This creates a standard, standalone model that is easier for inference.\n",
    "policy_model = policy_model.merge_and_unload()\n",
    "\n",
    "# Move the final model to the GPU and set it to evaluation mode.\n",
    "policy_model.to(device)\n",
    "policy_model.eval()\n",
    "print(f\"\\nSuccessfully loaded PPO-tuned policy model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604ae183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded tokenizers and reward model.\n",
      "\n",
      "Loaded 333 examples for evaluation.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Tokenizers and Reward Model ---\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(PPO_MODEL_PATH)\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(RM_PATH)\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(RM_PATH)\n",
    "reward_model.to(device)\n",
    "reward_model.eval()\n",
    "print(\"Successfully loaded tokenizers and reward model.\")\n",
    "\n",
    "# --- Load Evaluation Data ---\n",
    "# Load the test set that you saved during training.\n",
    "eval_df = pd.read_csv(EVAL_DATA)\n",
    "\n",
    "#Add instruction to every query\n",
    "eval_df['query'] = \"Vereinfache diesen Satz: \" + eval_df['query']\n",
    "\n",
    "# --- Randomly sample X% of the data ---\n",
    "# The `frac=0.1` parameter specifies that you want 10% of the rows.\n",
    "# `random_state=42` ensures you get the exact same random sample every time.\n",
    "eval_df = eval_df.sample(frac=EVAL_DATA_SUBSET, random_state=42)\n",
    "print(f\"\\nLoaded {len(eval_df)} examples for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332b362",
   "metadata": {},
   "source": [
    "### Step 2: Generate and Evaluate Responses\n",
    "#### Now that everything is loaded, you can generate new simplifications for your evaluation prompts and score them with your reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad4c7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  3. GENERATE & EVALUATE RESPONSES\n",
    "# ==============================================================================\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Define Helper Function to Score Responses ---\n",
    "def compute_rewards_from_rm(responses: list[str]) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        inputs = reward_tokenizer(responses, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        rewards = reward_model(**inputs).logits.squeeze(-1)\n",
    "    return rewards\n",
    "\n",
    "# --- Generation Settings ---\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True,\n",
    "    \"pad_token_id\": policy_tokenizer.eos_token_id, \"max_new_tokens\": 60,\n",
    "    \"repetition_penalty\": 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75045e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses with the PPO-tuned model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c8946a0df14486a75899a45bb3c06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# --- Generate Responses for the Evaluation Set ---\n",
    "# We'll process in batches to avoid memory issues with very large eval sets.\n",
    "batch_size = 16\n",
    "ppo_responses = []\n",
    "\n",
    "print(\"\\nGenerating responses with the PPO-tuned model...\")\n",
    "for i in tqdm(range(0, len(eval_df), batch_size)):\n",
    "    batch = eval_df[i:i+batch_size]\n",
    "    prompts = batch[\"query\"].tolist()\n",
    "    \n",
    "    # Tokenize prompts and move to the correct device\n",
    "    inputs = policy_tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    # Generate responses\n",
    "    with torch.no_grad():\n",
    "        response_tensors = policy_model.generate(**inputs, **generation_kwargs)\n",
    "\n",
    "    # Decode only the newly generated part of the response\n",
    "    decoded_responses = policy_tokenizer.batch_decode(response_tensors[:, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    ppo_responses.extend(decoded_responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d715372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Summary ---\n",
      "Average Reward Score for PPO-tuned model: 0.8676\n",
      "\n",
      "--- Side-by-Side Comparison (Sample) ---\n",
      "Query (Original): Vereinfache diesen Satz: So nennt man den Trainer von der deutschen Fußball·National·Mannschaft.\n",
      "Baseline (V1):    So nennt man den Trainer von der deutschen Fußball·National·Mannschaft.\n",
      "PPO Response:      So nennt man den Trainer von der deutschen Fußball-National-Mannschaft. So nennt man den Trainer von der deutschen Fußball-Nationalmannschaft. Der Trainer von der deutschen Fußball-Nationalmannschaft. So nennt man den Trainer der deutschen Fußball-National\n",
      "PPO Reward Score: 0.8791\n",
      "--------------------------------------------------\n",
      "Query (Original): Vereinfache diesen Satz: Dabei erinnerte man an den so genannten D-Day vor 75 Jahren.\n",
      "Baseline (V1):    man hat Dabei an den so genannten D-Day vor 75 Jahren erinnert.\n",
      "PPO Response:      Der 6. Juni 1944 war der Tag, an dem die Alliierten in der Normandie als Alliierte in der Normandie. Der 6. Juni 1944 war der Tag, an dem die Alliierten in der Normandie als Alliierte\n",
      "PPO Reward Score: 0.9320\n",
      "--------------------------------------------------\n",
      "Query (Original): Vereinfache diesen Satz: Thunberg bekam den Preis für ihre Proteste für den Klima·Schutz.\n",
      "Baseline (V1):    Thunberg hat den Preis für ihre Proteste für den Klima·Schutz bekommen.\n",
      "PPO Response:      Sie ist 18 Jahre jung.\n",
      "Verwende im Sinne von Fragen/Untersuchungen: Thunberg bekam den Preis für ihre Proteste für den Klima·Schutz. Sie ist 18 Jahre jung. Thunberg bekam den Preis für ihre Proteste für\n",
      "PPO Reward Score: 0.9294\n",
      "--------------------------------------------------\n",
      "Query (Original): Vereinfache diesen Satz: Die Feuerwehr konnte aber verhindern, dass sich das Feuer ausbreitet.\n",
      "Baseline (V1):    Die Feuerwehr hat aber verhindern, dass sich das Feuer ausbreitet gekonnt.\n",
      "PPO Response:      Sie konnten das Feuer innerhalb von 45 Minuten löschen.\n",
      "Vereinfache diesen Satz: Die Feuerwehr hat das Feuer innerhalb von 45 Minuten gelöscht.\n",
      "Die Feuerwehr hat das Feuer innerhalb von 45 Minuten gelös\n",
      "PPO Reward Score: 0.8793\n",
      "--------------------------------------------------\n",
      "Query (Original): Vereinfache diesen Satz: 53000.\n",
      "Baseline (V1):    53000\n",
      "PPO Response:     000 Millionen Euro\n",
      "Monatsgehalt?\n",
      "Das ist eine Frage, die sich viele Menschen stellen. Doch die Antwort ist komplex und hängt von verschiedenen Faktoren ab. Das Monatsgehalt ist ein wichtiger Faktor bei der Entscheidung, ob man sich eine Wohnung le\n",
      "PPO Reward Score: 0.8364\n",
      "--------------------------------------------------\n",
      "Query (Original): Vereinfache diesen Satz: In Österreich will man dann im Jänner mit den Corona·Impfungen anfangen.\n",
      "Baseline (V1):    In Österreich will man dann im Jänner mit den Corona·Impfungen anfangen.\n",
      "PPO Response:      Ab Januar 2021 sollen die ersten Corona·Impfungen in Österreich getestet werden. Derzeit sind laut Bundesministerium für Gesundheit im Gesundheitsministerium 12.000 Impfdosen in der Lagerung. Der Impfstoff soll vora\n",
      "PPO Reward Score: 0.8597\n",
      "--------------------------------------------------\n",
      "Query (Original): Vereinfache diesen Satz: Im Bundes·Land Niederösterreich ist am Sonntag am Abend etwas Schlimmes passiert.\n",
      "Baseline (V1):    Im Bundes·Land Niederösterreich ist am Sonntag am Abend etwas Schlimmes passiert.\n",
      "PPO Response:      Ein nicht mehr sehr junges Frau·en·leben war am Sonntag im Bundes·Land Nieder·österreich am Sonntag abgebrochen. Derartige Schlimme Ereignisse passieren leider leider recht oft. Eine junge Frau aus der Umgebung der Stadt Wien ist\n",
      "PPO Reward Score: 0.8962\n",
      "--------------------------------------------------\n",
      "Query (Original): Vereinfache diesen Satz: Der österreichische Schi·Star Marcel Hirscher beendet seine Karriere.\n",
      "Baseline (V1):    Der österreichische Schi·Star Marcel Hirscher beendet seine Karriere.\n",
      "PPO Response:      In einem – für die Medien – seltsamen Statement, das in einem Interview der österreichischen Tageszeitung “Der Standard” am Montag (20. November) veröffentlicht wurde, gab Hirscher bekannt, dass er seine Karriere als Schi-Sportler be\n",
      "PPO Reward Score: 0.9065\n",
      "--------------------------------------------------\n",
      "Query (Original): Vereinfache diesen Satz: Nicht alle Menschen fanden das gut.\n",
      "Baseline (V1):    Nicht alle Menschen hat das gut gefunden.\n",
      "PPO Response:      The sentence is similar to the English original, but it is not as clear. It can be shortened with the same meaning: Nur einige fanden das gut.\n",
      "The sentence is similar to the English original, but it is not as clear. It can be shortened with the same meaning: Nur einige f\n",
      "PPO Reward Score: 0.8616\n",
      "--------------------------------------------------\n",
      "Query (Original): Vereinfache diesen Satz: Der Mann machte das mit Absicht.\n",
      "Baseline (V1):    Der Mann hat das mit Absicht gemacht.\n",
      "PPO Response:      Er zündete den Feuerlöscher an, als er den Brand in seinem Haus im Süden der Stadt entdeckte. Die Feuerwehr kam schnell und löschte den Brand. Als die Feuerwehrleute das Haus verließen, kam der Mann bei\n",
      "PPO Reward Score: 0.9117\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add the new responses and their reward scores to the DataFrame\n",
    "eval_df[\"ppo_response\"] = ppo_responses\n",
    "eval_df[\"ppo_reward_score\"] = compute_rewards_from_rm(ppo_responses).cpu().numpy()\n",
    "\n",
    "# Save the DataFrame with the new responses and scores to a CSV file\n",
    "eval_df.to_csv(\"ppo_evaluation_results.csv\", index=False)\n",
    "\n",
    "# --- Print Final Results ---\n",
    "print(\"\\n--- Evaluation Summary ---\")\n",
    "# Quantitative Metric: It calculates the average reward score across all the generated responses in your evaluation set.\n",
    "print(f\"Average Reward Score for PPO-tuned model: {eval_df['ppo_reward_score'].mean():.4f}\")\n",
    "\n",
    "# Display a sample of the results for qualitative review\n",
    "# Qualitative Evaluation: It prints a random sample of side-by-side comparisons of the original query, \n",
    "# the baseline simplification, and the new PPO response. \n",
    "# This is a crucial step for you to manually assess the actual quality and style of the generated text.\n",
    "print(\"\\n--- Side-by-Side Comparison (Sample) ---\")\n",
    "for index, row in eval_df.sample(n=10, random_state=42).iterrows():\n",
    "    print(f\"Query (Original): {row['query']}\")\n",
    "    print(f\"Baseline (V1):    {row['final_simplification']}\")\n",
    "    print(f\"PPO Response:     {row['ppo_response']}\")\n",
    "    print(f\"PPO Reward Score: {row['ppo_reward_score']:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
