{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6077636",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b482280c",
   "metadata": {},
   "source": [
    "Start with setting variables for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b874c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_DATA = \"sft_split_dataset/eval.csv\"\n",
    "EVAL_DATA_SUBSET = 1 # 1 for whole df, 0.1 for only 10% \n",
    "\n",
    "# Check variable names before Running\n",
    "## 1) check query adjustment as well! SIMPLE vs DETAILED Query cmd+f: eval_df['query']\n",
    "### 2) adjust the file name for eval output, defines folder name\n",
    "SAVE_EVAL_SPECS = \"PPO_model_2K_4E_simple_q_28\"\n",
    "# 3) check the which model to be evaluated\n",
    "PPO_TO_EVALUATE = \"PPO_model_2K_4E_simple_q_28\"\n",
    "\n",
    "\n",
    "#1A \"PPO_model_2K_4E_simple_q_28\" - Simple Query\n",
    "#1B \"PPO_model_2K_4E_28\" Detailed Query\n",
    "#2A \"PPO_model_2K_4E_SFT\" -- KL=0.05\n",
    "#2B \"PPO_model_2K_4E_SFT_KL_variation\" -- KL=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c168b0",
   "metadata": {},
   "source": [
    "### Step 1: Setup and Loading the Final Model\n",
    "The most important step is loading the model correctly: Because the training happened with LoRA, first the original base model needs to be loaded and then add the saved adapter weights on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb83771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Configure Hardware Device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c72793",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE PATH AND LOAD REQUIRED COMPONENTS\n",
    "# The original base model your PPO model was fine-tuned from.\n",
    "BASE_MODEL_ID = \"frhew/sigdial_ft_a2\"\n",
    "# The path to your saved PPO model, which contains the LoRA adapter weights.\n",
    "PPO_MODEL_PATH = PPO_TO_EVALUATE\n",
    "# The path to your custom-trained reward model.\n",
    "RM_PATH = \"rm_out_rules_heavy_final\"\n",
    "\n",
    "# --- Load Base Model \n",
    "# First, load the original, pre-trained base model in bfloat16 for memory efficiency.\n",
    "base_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# --- Load the PEFT Model (Policy Model\n",
    "# Now, loading the LoRA adapters from your saved directory and applying them to the base model.\n",
    "policy_model = PeftModel.from_pretrained(base_model, PPO_MODEL_PATH)\n",
    "# Merge the adapter weights into the base model and unload the PEFT model.\n",
    "# This creates a standard, standalone model that is easier for inference.\n",
    "policy_model = policy_model.merge_and_unload()\n",
    "\n",
    "# Move the final model to the GPU and set it to evaluation mode.\n",
    "policy_model.to(device)\n",
    "policy_model.eval()\n",
    "print(f\"\\nSuccessfully loaded PPO-tuned policy model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ae183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Tokenizers and Reward Model ---\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(PPO_MODEL_PATH)\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(RM_PATH)\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(RM_PATH)\n",
    "reward_model.to(device)\n",
    "reward_model.eval()\n",
    "print(\"Successfully loaded tokenizers and reward model.\")\n",
    "\n",
    "# --- Load Evaluation Data\n",
    "# Load the test set that was saved during training.\n",
    "eval_df = pd.read_csv(EVAL_DATA)\n",
    "\n",
    "#Add instruction to every query, keep EITHER uncommented\n",
    "#### Simple Query \n",
    "eval_df['query'] = \"Vereinfache diesen Satz: \" + eval_df['original']\n",
    "\n",
    "#### Instruction specific query\n",
    "#eval_df['query'] = \"Vereinfache den folgenden Satz nach Leichter-Sprache-Regeln: \" + eval_df['original']\n",
    "\n",
    "# --- Randomly sample X% of the data\n",
    "eval_df = eval_df.sample(frac=EVAL_DATA_SUBSET, random_state=42)\n",
    "print(f\"\\nLoaded {len(eval_df)} examples for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332b362",
   "metadata": {},
   "source": [
    "### Step 2: Generate and Evaluate Responses\n",
    "\n",
    "- Generating new simplifications for the evaluation prompts and scoring them with the trained RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Helper Function to Score Responses\n",
    "def compute_rewards_from_rm(responses: list[str]) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        inputs = reward_tokenizer(responses, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        rewards = reward_model(**inputs).logits.squeeze(-1)\n",
    "    return rewards\n",
    "\n",
    "# --- Generation Settings\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True,\n",
    "    \"pad_token_id\": policy_tokenizer.eos_token_id, \"max_new_tokens\": 40,\n",
    "    \"repetition_penalty\": 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75045e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate Responses for the Evaluation Set\n",
    "# Proceeding in batches to avoid potential memory issues.\n",
    "batch_size = 16\n",
    "ppo_responses = []\n",
    "\n",
    "print(\"\\nGenerating responses with the PPO-tuned model...\")\n",
    "for i in tqdm(range(0, len(eval_df), batch_size)):\n",
    "    batch = eval_df[i:i+batch_size]\n",
    "    prompts = batch[\"query\"].tolist()\n",
    "    \n",
    "    # Tokenize prompts and move to the correct device\n",
    "    inputs = policy_tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    # Generate responses\n",
    "    with torch.no_grad():\n",
    "        response_tensors = policy_model.generate(**inputs, **generation_kwargs)\n",
    "\n",
    "    # Decode only the newly generated part of the response\n",
    "    decoded_responses = policy_tokenizer.batch_decode(response_tensors[:, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    ppo_responses.extend(decoded_responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new responses and their reward scores to the DataFrame\n",
    "eval_df[\"ppo_response\"] = ppo_responses\n",
    "eval_df[\"ppo_reward_score\"] = compute_rewards_from_rm(ppo_responses).cpu().numpy()\n",
    "\n",
    "# Save the DataFrame with the new responses and scores to a CSV file\n",
    "eval_df.to_csv(f\"ppo_evaluation_result_{SAVE_EVAL_SPECS}.csv\", index=False)\n",
    "\n",
    "# --- Print Final Results\n",
    "print(\"\\n--- Evaluation Summary ---\")\n",
    "# Quantitative Metric: It calculates the average reward score across all the generated responses in your evaluation set.\n",
    "print(f\"Average Reward Score for PPO-tuned model: {eval_df['ppo_reward_score'].mean():.4f}\")\n",
    "\n",
    "# Display a sample of the results for qualitative review\n",
    "# Qualitative Evaluation: It prints a random sample of side-by-side comparisons of the original query, \n",
    "# the baseline simplification, and the new PPO response. \n",
    "print(\"\\n--- Side-by-Side Comparison (Sample) ---\")\n",
    "for index, row in eval_df.sample(n=50, random_state=42).iterrows():\n",
    "    print(f\"Query (Original): {row['query']}\")\n",
    "    #print(f\"Baseline (V1):    {row['final_simplification']}\") rows did not get renamed\n",
    "    print(f\"Baseline (V1):    {row['simplified']}\")\n",
    "    print(f\"PPO Response:     {row['ppo_response']}\")\n",
    "    print(f\"PPO Reward Score: {row['ppo_reward_score']:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
