{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8652161e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0926 08:48:18.262000 17949 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/trl-env/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "from trl import PPOTrainer\n",
    "from tqdm.auto import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53ef9960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae3b462051441b3a2a5f5572c3397f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'frhew/sigdial_ft_a2', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  1. LOAD MODELS AND TOKENIZERS\n",
    "# ==============================================================================\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Define Model Paths ---\n",
    "# The policy model is the base LLM we want to fine-tune with PPO.\n",
    "#POLICY_MODEL_ID = \"dbmdz/german-gpt2\" \n",
    "#TODO: baseline=non-fine tuned model meta-llama/Meta-Llama-3-8B-Instruct\n",
    "POLICY_MODEL_ID = \"frhew/sigdial_ft_a2\" \n",
    "\n",
    "\n",
    "# This is the path to your custom-trained \"rules_heavy\" reward model.\n",
    "RM_PATH = \"rm_out_rules_heavy_final\"\n",
    "\n",
    "# --- Load Policy and Reference Models ---\n",
    "# The policy model is loaded with a value head for PPO training.\n",
    "# policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(POLICY_MODEL_ID)\n",
    "# the above is loaded later with lora\n",
    "\n",
    "# The reference model is a frozen copy of the original policy.\n",
    "# --- Load the standard reference model (without LoRA) ---\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "                POLICY_MODEL_ID,\n",
    "                torch_dtype=torch.bfloat16, # <-- Use lower precision\n",
    ")\n",
    "\n",
    "# the above line takes 30min tot downlaod 7 shareds each close to 5GB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b926beb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Load Tokenizers ---\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(POLICY_MODEL_ID, padding_side='left') # Add padding_side\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(RM_PATH, padding_side='left')     # Add padding_side\n",
    "\n",
    "print(\"All models and tokenizers loaded successfully.\")\n",
    "\n",
    "# Set padding token for both tokenizers if it's not already set.\n",
    "#TODO assess which padding is required \n",
    "if policy_tokenizer.pad_token is None:\n",
    "    policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "if reward_tokenizer.pad_token is None:\n",
    "    reward_tokenizer.pad_token = reward_tokenizer.eos_token\n",
    "\n",
    "\"\"\"\n",
    "# --- Load Tokenizers ---\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(POLICY_MODEL_ID)\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(RM_PATH)\n",
    "\n",
    "print(\"All models and tokenizers loaded successfully.\")\n",
    "\n",
    "# Set padding token for the policy tokenizer if it's not already set.\n",
    "if policy_tokenizer.pad_token is None:\n",
    "    policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d079934",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Load Custom Reward Model ---\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(RM_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099603bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Configure Hardware Device (CPU vs. Apple Silicon GPU) ---\n",
    "# Check for the availability of Apple's Metal Performance Shaders (MPS) for GPU\n",
    "# acceleration and set the global device accordingly.\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8236d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Move the reward model to the correct device and set it to evaluation mode.\n",
    "reward_model.to(device) # 'device' was set to \"mps\" in a previous cell\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff5c56",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  2. PREPARE DATASET FOR PPO\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset containing the prompts for the policy model.\n",
    "df = pd.read_csv(\"data/ordered_simplifications_with_rules.csv\", index_col=0)\n",
    "\n",
    "# The 'query' is the prompt given to the PPO model. \n",
    "# We'll use the original complex sentence as the prompt.\n",
    "df.rename(columns={\"original_sentence\": \"query\"}, inplace=True)\n",
    "# df = df.sample(n=64, random_state=42) #Uncomment for Sampled Data with low number of datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd93184",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Create a train/eval set\n",
    "train_df, eval_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "print(f\"Training set size: {len(train_df)}, Evaluation set size: {len(eval_df)}\")\n",
    "\n",
    "# For this demonstration, we'll use a small sample. For a full run, use the entire df.\n",
    "ppo_dataset = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3405269c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def format_query(example):\n",
    "    \"\"\"\n",
    "    Applies the instruction template \"Vereinfache diesen Satz:\" \n",
    "    to each query in the dataset.\n",
    "    \"\"\"\n",
    "    # The 'query' in the example is the original, complex sentence.\n",
    "    example['query'] = f\"Vereinfache diesen Satz: {example['query']}\"\n",
    "    return example\n",
    "\n",
    "# Apply the formatting function to every example in the dataset.\n",
    "# The .map() function processes each row and updates the 'query' column.\n",
    "###applying this on EVAL happens later on\n",
    "ppo_dataset = ppo_dataset.map(format_query)\n",
    "\n",
    "# You can now check an example to see the new format\n",
    "print(\"Example of a formatted query:\")\n",
    "print(ppo_dataset[0]['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53777f8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create the Hugging Face Dataset from a smaller, random sample.\n",
    "# Using 1024-2048 prompts is often sufficient for a PPO run.\n",
    "# Using a smaller, diverse set of prompts is often sufficient to train a robust policy, \n",
    "# as it provides enough variety to generate the experiences the model needs to learn from, \n",
    "# without the massive memory cost.\n",
    "#4,096, 6,144\n",
    "ppo_dataset = Dataset.from_pandas(train_df.sample(n=2048, random_state=42))\n",
    "\n",
    "def tokenize_query(examples):\n",
    "    \"\"\"Tokenizes the 'query' column for the PPO trainer.\"\"\"\n",
    "    # Remove `padding=\"max_length\"`. The PPOTrainer's dataloader will handle padding dynamically.\n",
    "    #return policy_tokenizer(examples[\"query\"], truncation=True, max_length=60)\n",
    "    # \"\"\"Tokenizes the 'query' column for the PPO trainer.\"\"\"\n",
    "    return policy_tokenizer(examples[\"query\"], \n",
    "                            truncation=True, \n",
    "                            padding=\"max_length\", \n",
    "                            max_length=40)\n",
    "\n",
    "# Tokenize the dataset and format it for PyTorch.\n",
    "ppo_dataset = ppo_dataset.map(tokenize_query, batched=True,\n",
    "                              remove_columns=['final_simplification', 'applied_rules', 'uid', 'query'])\n",
    "ppo_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf48da",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#print(ppo_dataset)\n",
    "#print(ppo_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8955b49",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#ppo_dataset ## TODO remove unnecessary cols\n",
    "\n",
    "# example:\n",
    "# tokenized_dataset = dataset.map(tokenize_function, \n",
    "#                                 batched=True,\n",
    "#                                 remove_columns=['final_simplification', 'applied_rules', 'uid', 'query']\n",
    "#                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14368661",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  3. CONFIGURE PPO (WITH LORA)\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Current PPO script is not performing hyperparameter tuning. \n",
    "It is running a single training session with one fixed set of parameters defined in your PPOConfig.\n",
    "\n",
    "Unlike the standard Trainer used for your reward model, \n",
    "the PPOTrainer does not have a built-in .hyperparameter_search() method. \n",
    "You have to implement the search logic yourself with a loop.\n",
    "\n",
    "\"\"\"\n",
    "from trl import PPOConfig, PPOTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "# --- Define the LoRA configuration ---\n",
    "# This object defines the configuration for LoRA (Low-Rank Adaptation),\n",
    "# a technique for efficient model fine-tuning.\n",
    "lora_config = LoraConfig(\n",
    "    # r: The rank of the LoRA matrices. This is the most important parameter.\n",
    "    # It controls the number of trainable parameters. A higher rank means more\n",
    "    # expressive power but also more parameters to train. Common values are 8, 16, 32.\n",
    "    r=16,\n",
    "    \n",
    "    # lora_alpha: A scaling factor for the LoRA updates. It's like a learning\n",
    "    # rate for the adapter layers. A common practice is to set alpha to twice the rank (2 * r).\n",
    "    lora_alpha=32,\n",
    "    \n",
    "    # lora_dropout: Applies dropout regularization to the LoRA layers. This helps\n",
    "    # prevent overfitting by randomly setting a fraction of adapter activations to zero.\n",
    "    lora_dropout=0.05,\n",
    "    \n",
    "    # bias: Determines which bias parameters in the model are trained. \"none\" is a\n",
    "    # common setting that freezes all bias terms and only trains the new LoRA weights.\n",
    "    bias=\"none\",\n",
    "    \n",
    "    # task_type: Specifies the type of model you are adapting. This is crucial for\n",
    "    # the PEFT library to correctly identify and modify the right layers.\n",
    "    # \"CAUSAL_LM\" is correct for GPT-style models used for text generation.\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99795051",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # --- Load the Policy Model with PEFT config ---\n",
    "# # TRL's special model class can directly accept a peft_config.\n",
    "# # This ensures the model is created in the exact format the PPOTrainer expects.\n",
    "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    POLICY_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16, # <-- Use lower precision\n",
    "    peft_config=lora_config  # <-- Pass the LoRA config here during loading\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to trade a little computation for a lot of memory.\n",
    "#policy_model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e4292",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Define the PPOConfig  ---\n",
    "# A recommended, stable set of hyperparameters for a single run without a full search.\n",
    "config = PPOConfig(\n",
    "    model_name=POLICY_MODEL_ID,\n",
    "    # A learning rate of 1.4e-5 is a common and effective starting point for fine-tuning.\n",
    "    learning_rate=1.4e-5,\n",
    "    # The number of times you iterate over the collected PPO experiences in each optimization step.\n",
    "    ppo_epochs=5, #default is 4\n",
    "    # The number of prompts to collect before performing an optimization.\n",
    "    batch_size=16, #32,\n",
    "    # The PPO batch is split into smaller mini-batches for the update.\n",
    "    mini_batch_size=4, #8,\n",
    "\n",
    "    # --- MEMORY FIX 2: Add Gradient Accumulation ---\n",
    "    # This will process 4 mini-batches before performing a model update.\n",
    "    # Effective Batch Size = 4 (mini_batch_size) * 4 (accumulation_steps) = 16\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    # Disables external logging integrations like WandB.\n",
    "    log_with=None,\n",
    "    ### --- FIX\n",
    "    # Use the full KL penalty to ensure stability with LoRA-adapted modles. This prevents the negative KL divergence by applying the KL calculation more robustly.\n",
    "    # Use a KL penalty to stop the model from deviating too far from the original.\n",
    "    kl_penalty=\"full\", #changed frol kl\n",
    "    # A slightly lower target KL can improve stability and prevent the model from changing too drastically.\n",
    "    target_kl=0.05,\n",
    "    # --- Recommended additions for stability ---\n",
    "    # Normalizes the reward scores, which is a key practice for stable PPO training.\n",
    "    use_score_scaling=True,\n",
    "    # The coefficient for the value function loss in the PPO update.\n",
    "    vf_coef=0.1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd3181",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # ==============================================================================\n",
    "# #  4. INITIALIZE THE PPO TRAINER (WITH DATA COLLATOR)\n",
    "# # ==============================================================================\n",
    "\n",
    "# # --- 1. Initialize the PPOTrainer with all our components ---\n",
    "\n",
    "# # THE FIX: Create a data collator that will dynamically pad each batch.\n",
    "# # It uses the policy_tokenizer to know what padding token to use.\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=policy_tokenizer)\n",
    "\n",
    "# ppo_trainer = PPOTrainer(\n",
    "#     config=config,\n",
    "#     model=policy_model,\n",
    "#     ref_model=ref_model,\n",
    "#     tokenizer=policy_tokenizer,\n",
    "#     dataset=ppo_dataset,\n",
    "#     data_collator=data_collator  # <-- NEW: Add the data collator here\n",
    "# )\n",
    "\n",
    "# # ... (The rest of your code for generation_kwargs and the training loop remains the same) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7ffdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Initialize the PPOTrainer ---\n",
    "# Initialize the PPOTrainer with all our components.\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=config,\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=policy_tokenizer,\n",
    "    dataset=ppo_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97d8a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  4. THE PPO TRAINING LOOP\n",
    "# ==============================================================================\n",
    "from trl import PPOTrainer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Generation settings for creating the simplified responses.\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": policy_tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 40, # Control the length of the simplification\n",
    "}\n",
    "\n",
    "# The main training loop\n",
    "for epoch in range(config.ppo_epochs):\n",
    "    for batch in tqdm(ppo_trainer.dataloader, f\"PPO Epoch {epoch+1}\"):\n",
    "        \n",
    "        # A. Get prompts (queries) from the batch.\n",
    "        query_tensors = batch['input_ids']\n",
    "\n",
    "        # B. Generate responses from the policy model.\n",
    "        # THE FIX: Convert the 2D batch tensor into a list of 1D tensors.\n",
    "        queries_list = [q for q in query_tensors]\n",
    "        response_tensors = ppo_trainer.generate(queries_list, **generation_kwargs)\n",
    "        batch['response'] = policy_tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "\n",
    "        # C. Score the responses with your custom reward model.\n",
    "        # This is where the \"Feedback\" from your diagram happens.\n",
    "        texts_to_score = batch['response']\n",
    "        rewards = []\n",
    "        with torch.no_grad():\n",
    "            # Tokenize for the reward model\n",
    "            inputs = reward_tokenizer(texts_to_score, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            # Get the raw score (logits)\n",
    "            reward_logits = reward_model(**inputs).logits.squeeze(-1)\n",
    "            # Store rewards as a list of PyTorch tensors.\n",
    "            rewards = [r for r in reward_logits]\n",
    "            \n",
    "        # D. Perform the PPO optimization step.\n",
    "        # This updates the policy model's weights based on the rewards, while\n",
    "        # also calculating the KL penalty against the reference model.\n",
    "        #stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        # Use the 'queries_list' you already created for the generate step\n",
    "        stats = ppo_trainer.step(queries_list, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "print(\"PPO Training Finished!\")\n",
    "\n",
    "# Save the final tuned model.\n",
    "ppo_trainer.save_pretrained(\"my_ppo_tuned_model_V26.09\")\n",
    "print(\"Model saved to 'my_ppo_tuned_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ee57b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_df.info()\n",
    "train_df.to_csv('train_df_26Sep.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b99810",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sampled_train_df = ppo_dataset.to_pandas()\n",
    "sampled_train_df.info()\n",
    "train_df.to_csv('sampled_train_df_26Sep.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc631f05",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "eval_df.info()\n",
    "eval_df.to_csv('eval_df_26Sep.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2303f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825262a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04cd2cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Define the generation settings used during training.\n",
    "# generation_kwargs = {\n",
    "#     \"min_length\": -1,\n",
    "#     \"top_k\": 0.0,\n",
    "#     \"top_p\": 1.0,\n",
    "#     \"do_sample\": True,\n",
    "#     \"pad_token_id\": policy_tokenizer.eos_token_id,\n",
    "#     \"max_new_tokens\": 40,\n",
    "# }\n",
    "\n",
    "# # Define the function to score responses with your reward model.\n",
    "# def compute_rewards_from_rm(responses: list[str]) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Computes rewards for a list of strings using the trained regression reward model.\n",
    "#     \"\"\"\n",
    "#     with torch.no_grad():\n",
    "#         # Tokenize the responses for the reward model.\n",
    "#         inputs = reward_tokenizer(\n",
    "#             responses,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#         ).to(device)\n",
    "        \n",
    "#         # Get the reward model's output logits (the scalar reward values).\n",
    "#         rewards = reward_model(**inputs).logits.squeeze(-1)\n",
    "        \n",
    "#     return rewards\n",
    "\n",
    "# print(\"\\n--- Setup complete. You can now run your evaluation script. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af101a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# print(\"\\n--- Evaluating and Logging Final Model Performance ---\")\n",
    "\n",
    "# # Take a small, fresh sample from your original dataframe for evaluation\n",
    "# # Using a different random_state ensures we get a different set of prompts\n",
    "\n",
    "# eval_df_small = eval_df.sample(n=50, random_state=49)\n",
    "# eval_dataset = Dataset.from_pandas(eval_df_small)\n",
    "\n",
    "# # Tokenize the evaluation prompts\n",
    "# # Note: We are not using the dataloader here, just tokenizing a small batch\n",
    "# eval_prompts = policy_tokenizer(\n",
    "#     eval_df[\"query\"].tolist(), # Use the pandas DataFrame here\n",
    "#     return_tensors=\"pt\",\n",
    "#     padding=True,\n",
    "#     truncation=True,\n",
    "#     max_length = 60\n",
    "# )\n",
    "# eval_queries_list = [q.to(ppo_trainer.accelerator.device) for q in eval_prompts['input_ids']] # Ensure tensors are on the correct device\n",
    "\n",
    "# # Generate responses with the FINAL trained model\n",
    "# # We use torch.no_grad() for efficiency as we are not training\n",
    "# with torch.no_grad():\n",
    "#     eval_response_tensors = ppo_trainer.generate(eval_queries_list, **generation_kwargs)\n",
    "\n",
    "# # THE FIX: Isolate the generated tokens before decoding\n",
    "# # ==============================================================================\n",
    "# generated_tokens = []\n",
    "# # Get the original prompts' attention mask to find their true, unpadded lengths\n",
    "# prompt_attention_mask = eval_prompts['attention_mask']\n",
    "\n",
    "# for i, response_tensor in enumerate(eval_response_tensors):\n",
    "#     # Find the actual length of the prompt by summing its attention mask\n",
    "#     prompt_len = torch.sum(prompt_attention_mask[i])\n",
    "    \n",
    "#     # Slice the individual response tensor to get only the newly generated part\n",
    "#     generated_part = response_tensor[prompt_len:]\n",
    "#     generated_tokens.append(generated_part)\n",
    "\n",
    "# # Decode only the generated part\n",
    "# eval_responses = policy_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "# # ==============================================================================\n",
    "\n",
    "# # Score the final responses with the reward model\n",
    "# eval_rewards = compute_rewards_from_rm(eval_responses)\n",
    "\n",
    "# # Print the results in a clean format\n",
    "# print(\"\\n--- Final Model Generations ---\")\n",
    "# for i in range(len(eval_dataset[\"query\"])):\n",
    "#     print(f\"Query:    {eval_dataset['query'][i]}\")\n",
    "#     print(f\"Response: {eval_responses[i]}\")\n",
    "#     print(f\"Reward:   {eval_rewards[i].item():.4f}\")\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b0bf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
