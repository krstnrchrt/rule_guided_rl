{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8652161e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0926 08:53:42.611000 18564 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/trl-env/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "from trl import PPOTrainer\n",
    "from tqdm.auto import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53ef9960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d814fb81d37b444285d16aae79a96af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'frhew/sigdial_ft_a2', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  1. LOAD MODELS AND TOKENIZERS\n",
    "# ==============================================================================\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Define Model Paths ---\n",
    "# The policy model is the base LLM we want to fine-tune with PPO.\n",
    "#POLICY_MODEL_ID = \"dbmdz/german-gpt2\" \n",
    "#TODO: baseline=non-fine tuned model meta-llama/Meta-Llama-3-8B-Instruct\n",
    "POLICY_MODEL_ID = \"frhew/sigdial_ft_a2\" \n",
    "\n",
    "\n",
    "# This is the path to your custom-trained \"rules_heavy\" reward model.\n",
    "RM_PATH = \"rm_out_rules_heavy_final\"\n",
    "\n",
    "# --- Load Policy and Reference Models ---\n",
    "# The policy model is loaded with a value head for PPO training.\n",
    "# policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(POLICY_MODEL_ID)\n",
    "# the above is loaded later with lora\n",
    "\n",
    "# The reference model is a frozen copy of the original policy.\n",
    "# --- Load the standard reference model (without LoRA) ---\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "                POLICY_MODEL_ID,\n",
    "                torch_dtype=torch.bfloat16, # <-- Use lower precision\n",
    ")\n",
    "\n",
    "# the above line takes 30min tot downlaod 7 shareds each close to 5GB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b926beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models and tokenizers loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# --- Load Tokenizers ---\\npolicy_tokenizer = AutoTokenizer.from_pretrained(POLICY_MODEL_ID)\\nreward_tokenizer = AutoTokenizer.from_pretrained(RM_PATH)\\n\\nprint(\"All models and tokenizers loaded successfully.\")\\n\\n# Set padding token for the policy tokenizer if it\\'s not already set.\\nif policy_tokenizer.pad_token is None:\\n    policy_tokenizer.pad_token = policy_tokenizer.eos_token\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Load Tokenizers ---\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(POLICY_MODEL_ID, padding_side='left') # Add padding_side\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(RM_PATH, padding_side='left')     # Add padding_side\n",
    "\n",
    "print(\"All models and tokenizers loaded successfully.\")\n",
    "\n",
    "# Set padding token for both tokenizers if it's not already set.\n",
    "#TODO assess which padding is required \n",
    "if policy_tokenizer.pad_token is None:\n",
    "    policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "if reward_tokenizer.pad_token is None:\n",
    "    reward_tokenizer.pad_token = reward_tokenizer.eos_token\n",
    "\n",
    "\"\"\"\n",
    "# --- Load Tokenizers ---\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(POLICY_MODEL_ID)\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(RM_PATH)\n",
    "\n",
    "print(\"All models and tokenizers loaded successfully.\")\n",
    "\n",
    "# Set padding token for the policy tokenizer if it's not already set.\n",
    "if policy_tokenizer.pad_token is None:\n",
    "    policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d079934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Custom Reward Model ---\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(RM_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099603bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# --- Configure Hardware Device (CPU vs. Apple Silicon GPU) ---\n",
    "# Check for the availability of Apple's Metal Performance Shaders (MPS) for GPU\n",
    "# acceleration and set the global device accordingly.\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a8236d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(31102, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the reward model to the correct device and set it to evaluation mode.\n",
    "reward_model.to(device) # 'device' was set to \"mps\" in a previous cell\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dff5c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  2. PREPARE DATASET FOR PPO\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset containing the prompts for the policy model.\n",
    "df = pd.read_csv(\"data/ordered_simplifications_with_rules.csv\", index_col=0)\n",
    "\n",
    "# The 'query' is the prompt given to the PPO model. \n",
    "# We'll use the original complex sentence as the prompt.\n",
    "df.rename(columns={\"original_sentence\": \"query\"}, inplace=True)\n",
    "# df = df.sample(n=64, random_state=42) #Uncomment for Sampled Data with low number of datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bd93184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 9440, Evaluation set size: 1667\n"
     ]
    }
   ],
   "source": [
    "#Create a train/eval set\n",
    "train_df, eval_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "print(f\"Training set size: {len(train_df)}, Evaluation set size: {len(eval_df)}\")\n",
    "\n",
    "# For this demonstration, we'll use a small sample. For a full run, use the entire df.\n",
    "ppo_dataset = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3405269c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a9cb00b7dd4e42916eae8035889dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a formatted query:\n",
      "Vereinfache diesen Satz: Auch die 17 Jahre alte Greta Thunberg aus Schweden hielt am Dienstag eine Rede.\n"
     ]
    }
   ],
   "source": [
    "def format_query(example):\n",
    "    \"\"\"\n",
    "    Applies the instruction template \"Vereinfache diesen Satz:\" \n",
    "    to each query in the dataset.\n",
    "    \"\"\"\n",
    "    # The 'query' in the example is the original, complex sentence.\n",
    "    example['query'] = f\"Vereinfache diesen Satz: {example['query']}\"\n",
    "    return example\n",
    "\n",
    "# Apply the formatting function to every example in the dataset.\n",
    "# The .map() function processes each row and updates the 'query' column.\n",
    "###applying this on EVAL happens later on\n",
    "ppo_dataset = ppo_dataset.map(format_query)\n",
    "\n",
    "# You can now check an example to see the new format\n",
    "print(\"Example of a formatted query:\")\n",
    "print(ppo_dataset[0]['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53777f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc568215a7a5472d811f67e19130747a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the Hugging Face Dataset from a smaller, random sample.\n",
    "# Using 1024-2048 prompts is often sufficient for a PPO run.\n",
    "# Using a smaller, diverse set of prompts is often sufficient to train a robust policy, \n",
    "# as it provides enough variety to generate the experiences the model needs to learn from, \n",
    "# without the massive memory cost.\n",
    "#4,096, 6,144\n",
    "ppo_dataset = Dataset.from_pandas(train_df.sample(n=2048, random_state=42))\n",
    "\n",
    "def tokenize_query(examples):\n",
    "    \"\"\"Tokenizes the 'query' column for the PPO trainer.\"\"\"\n",
    "    # Remove `padding=\"max_length\"`. The PPOTrainer's dataloader will handle padding dynamically.\n",
    "    #return policy_tokenizer(examples[\"query\"], truncation=True, max_length=60)\n",
    "    # \"\"\"Tokenizes the 'query' column for the PPO trainer.\"\"\"\n",
    "    return policy_tokenizer(examples[\"query\"], \n",
    "                            truncation=True, \n",
    "                            padding=\"max_length\", \n",
    "                            max_length=40)\n",
    "\n",
    "# Tokenize the dataset and format it for PyTorch.\n",
    "ppo_dataset = ppo_dataset.map(tokenize_query, batched=True,\n",
    "                              remove_columns=['final_simplification', 'applied_rules', 'uid', 'query'])\n",
    "ppo_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37cf48da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ppo_dataset)\n",
    "#print(ppo_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8955b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppo_dataset ## TODO remove unnecessary cols\n",
    "\n",
    "# example:\n",
    "# tokenized_dataset = dataset.map(tokenize_function, \n",
    "#                                 batched=True,\n",
    "#                                 remove_columns=['final_simplification', 'applied_rules', 'uid', 'query']\n",
    "#                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14368661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  3. CONFIGURE PPO (WITH LORA)\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Current PPO script is not performing hyperparameter tuning. \n",
    "It is running a single training session with one fixed set of parameters defined in your PPOConfig.\n",
    "\n",
    "Unlike the standard Trainer used for your reward model, \n",
    "the PPOTrainer does not have a built-in .hyperparameter_search() method. \n",
    "You have to implement the search logic yourself with a loop.\n",
    "\n",
    "\"\"\"\n",
    "from trl import PPOConfig, PPOTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "# --- Define the LoRA configuration ---\n",
    "# This object defines the configuration for LoRA (Low-Rank Adaptation),\n",
    "# a technique for efficient model fine-tuning.\n",
    "lora_config = LoraConfig(\n",
    "    # r: The rank of the LoRA matrices. This is the most important parameter.\n",
    "    # It controls the number of trainable parameters. A higher rank means more\n",
    "    # expressive power but also more parameters to train. Common values are 8, 16, 32.\n",
    "    r=16,\n",
    "    \n",
    "    # lora_alpha: A scaling factor for the LoRA updates. It's like a learning\n",
    "    # rate for the adapter layers. A common practice is to set alpha to twice the rank (2 * r).\n",
    "    lora_alpha=32,\n",
    "    \n",
    "    # lora_dropout: Applies dropout regularization to the LoRA layers. This helps\n",
    "    # prevent overfitting by randomly setting a fraction of adapter activations to zero.\n",
    "    lora_dropout=0.05,\n",
    "    \n",
    "    # bias: Determines which bias parameters in the model are trained. \"none\" is a\n",
    "    # common setting that freezes all bias terms and only trains the new LoRA weights.\n",
    "    bias=\"none\",\n",
    "    \n",
    "    # task_type: Specifies the type of model you are adapting. This is crucial for\n",
    "    # the PEFT library to correctly identify and modify the right layers.\n",
    "    # \"CAUSAL_LM\" is correct for GPT-style models used for text generation.\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99795051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd87a8e80bd24829ae9a5a8e5a59cd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'peft.peft_model.PeftModelForCausalLM'> model is loaded from 'frhew/sigdial_ft_a2', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    }
   ],
   "source": [
    "# # --- Load the Policy Model with PEFT config ---\n",
    "# # TRL's special model class can directly accept a peft_config.\n",
    "# # This ensures the model is created in the exact format the PPOTrainer expects.\n",
    "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    POLICY_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16, # <-- Use lower precision\n",
    "    peft_config=lora_config  # <-- Pass the LoRA config here during loading\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to trade a little computation for a lot of memory.\n",
    "#policy_model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "957e4292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the PPOConfig  ---\n",
    "# A recommended, stable set of hyperparameters for a single run without a full search.\n",
    "config = PPOConfig(\n",
    "    model_name=POLICY_MODEL_ID,\n",
    "    # A learning rate of 1.4e-5 is a common and effective starting point for fine-tuning.\n",
    "    learning_rate=1.4e-5,\n",
    "    # The number of times you iterate over the collected PPO experiences in each optimization step.\n",
    "    ppo_epochs=5, #default is 4\n",
    "    # The number of prompts to collect before performing an optimization.\n",
    "    batch_size=16, #32,\n",
    "    # The PPO batch is split into smaller mini-batches for the update.\n",
    "    mini_batch_size=4, #8,\n",
    "\n",
    "    # --- MEMORY FIX 2: Add Gradient Accumulation ---\n",
    "    # This will process 4 mini-batches before performing a model update.\n",
    "    # Effective Batch Size = 4 (mini_batch_size) * 4 (accumulation_steps) = 16\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    # Disables external logging integrations like WandB.\n",
    "    log_with=None,\n",
    "    ### --- FIX\n",
    "    # Use the full KL penalty to ensure stability with LoRA-adapted modles. This prevents the negative KL divergence by applying the KL calculation more robustly.\n",
    "    # Use a KL penalty to stop the model from deviating too far from the original.\n",
    "    kl_penalty=\"full\", #changed frol kl\n",
    "    # A slightly lower target KL can improve stability and prevent the model from changing too drastically.\n",
    "    target_kl=0.05,\n",
    "    # --- Recommended additions for stability ---\n",
    "    # Normalizes the reward scores, which is a key practice for stable PPO training.\n",
    "    use_score_scaling=True,\n",
    "    # The coefficient for the value function loss in the PPO update.\n",
    "    vf_coef=0.1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59cd3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================================================================\n",
    "# #  4. INITIALIZE THE PPO TRAINER (WITH DATA COLLATOR)\n",
    "# # ==============================================================================\n",
    "\n",
    "# # --- 1. Initialize the PPOTrainer with all our components ---\n",
    "\n",
    "# # THE FIX: Create a data collator that will dynamically pad each batch.\n",
    "# # It uses the policy_tokenizer to know what padding token to use.\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=policy_tokenizer)\n",
    "\n",
    "# ppo_trainer = PPOTrainer(\n",
    "#     config=config,\n",
    "#     model=policy_model,\n",
    "#     ref_model=ref_model,\n",
    "#     tokenizer=policy_tokenizer,\n",
    "#     dataset=ppo_dataset,\n",
    "#     data_collator=data_collator  # <-- NEW: Add the data collator here\n",
    "# )\n",
    "\n",
    "# # ... (The rest of your code for generation_kwargs and the training loop remains the same) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88d7ffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize the PPOTrainer ---\n",
    "# Initialize the PPOTrainer with all our components.\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=config,\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=policy_tokenizer,\n",
    "    dataset=ppo_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc97d8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5408069791cf4fc8a7169bcb59c0cb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PPO Epoch 1:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfdc2e62eb34933a244e13544211077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PPO Epoch 2:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2957512d6834ca2b57a84b9d886c44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PPO Epoch 3:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffc41fc0fb44fdc9319075ab7703b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PPO Epoch 4:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9a7df093024914b1dd3825a850be07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PPO Epoch 5:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Training Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/trl-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to 'my_ppo_tuned_model'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/trl-env/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1407: UserWarning: Cannot retrieve user information assuming you are running in offline mode.\n",
      "  warnings.warn(\"Cannot retrieve user information assuming you are running in offline mode.\")\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  4. THE PPO TRAINING LOOP\n",
    "# ==============================================================================\n",
    "from trl import PPOTrainer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Generation settings for creating the simplified responses.\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": policy_tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 40, # Control the length of the simplification\n",
    "}\n",
    "\n",
    "# The main training loop\n",
    "for epoch in range(config.ppo_epochs):\n",
    "    for batch in tqdm(ppo_trainer.dataloader, f\"PPO Epoch {epoch+1}\"):\n",
    "        \n",
    "        # A. Get prompts (queries) from the batch.\n",
    "        query_tensors = batch['input_ids']\n",
    "\n",
    "        # B. Generate responses from the policy model.\n",
    "        # THE FIX: Convert the 2D batch tensor into a list of 1D tensors.\n",
    "        queries_list = [q for q in query_tensors]\n",
    "        response_tensors = ppo_trainer.generate(queries_list, **generation_kwargs)\n",
    "        batch['response'] = policy_tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "\n",
    "        # C. Score the responses with your custom reward model.\n",
    "        # This is where the \"Feedback\" from your diagram happens.\n",
    "        texts_to_score = batch['response']\n",
    "        rewards = []\n",
    "        with torch.no_grad():\n",
    "            # Tokenize for the reward model\n",
    "            inputs = reward_tokenizer(texts_to_score, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            # Get the raw score (logits)\n",
    "            reward_logits = reward_model(**inputs).logits.squeeze(-1)\n",
    "            # Store rewards as a list of PyTorch tensors.\n",
    "            rewards = [r for r in reward_logits]\n",
    "            \n",
    "        # D. Perform the PPO optimization step.\n",
    "        # This updates the policy model's weights based on the rewards, while\n",
    "        # also calculating the KL penalty against the reference model.\n",
    "        #stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        # Use the 'queries_list' you already created for the generate step\n",
    "        stats = ppo_trainer.step(queries_list, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "print(\"PPO Training Finished!\")\n",
    "\n",
    "# Save the final tuned model.\n",
    "ppo_trainer.save_pretrained(\"my_ppo_tuned_model_V26.09\")\n",
    "print(\"Model saved to 'my_ppo_tuned_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "873ee57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9440 entries, 6467 to 10863\n",
      "Data columns (total 3 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   query                 9440 non-null   object\n",
      " 1   final_simplification  9440 non-null   object\n",
      " 2   applied_rules         9440 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 295.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()\n",
    "train_df.to_csv('train_df_26Sep.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34b99810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2048 entries, 0 to 2047\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   input_ids       2048 non-null   object\n",
      " 1   attention_mask  2048 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 32.1+ KB\n"
     ]
    }
   ],
   "source": [
    "sampled_train_df = ppo_dataset.to_pandas()\n",
    "sampled_train_df.info()\n",
    "train_df.to_csv('sampled_train_df_26Sep.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc631f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1667 entries, 6973 to 11012\n",
      "Data columns (total 3 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   query                 1667 non-null   object\n",
      " 1   final_simplification  1667 non-null   object\n",
      " 2   applied_rules         1667 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 52.1+ KB\n"
     ]
    }
   ],
   "source": [
    "eval_df.info()\n",
    "eval_df.to_csv('eval_df_26Sep.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca2303f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>final_simplification</th>\n",
       "      <th>applied_rules</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6973</th>\n",
       "      <td>Sie können den Tumor auch mit Medikamenten bek...</td>\n",
       "      <td>Sie können den Tumor auch mit Medikamenten bek...</td>\n",
       "      <td>['normalize_verb_tense']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9309</th>\n",
       "      <td>Das war schon das 3. Mal in einer Woche, dass ...</td>\n",
       "      <td>Das ist schon das 3 Mal in einer Woche, dass i...</td>\n",
       "      <td>['convert_word_to_number', 'normalize_verb_ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>Im Fernsehen wird über die Wahl berichtet.</td>\n",
       "      <td>Man hat berichtet.</td>\n",
       "      <td>['convert_passive_to_active']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>In Hollywood \" wurde als \" Beste Komödie \" aus...</td>\n",
       "      <td>Man hat ausgezeichnet.</td>\n",
       "      <td>['convert_passive_to_active']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>Schwere Vorwürfe gegen Opern-Sänger Placido Do...</td>\n",
       "      <td>Schwere Vorwürfe gegen Opern·Sänger Placido Do...</td>\n",
       "      <td>['split_compound']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>Bundes-Kanzler Sebastian Kurz von der ÖVP woll...</td>\n",
       "      <td>Bundes-Kanzler Sebastian Kurz hat von der ÖVP ...</td>\n",
       "      <td>['normalize_verb_tense']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8991</th>\n",
       "      <td>Ab 1. Mai werden aber viele Sport-Plätze wiede...</td>\n",
       "      <td>Ab 1 Mai werden aber viele Sport·Plätze wieder...</td>\n",
       "      <td>['convert_word_to_number', 'split_compound', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12404</th>\n",
       "      <td>Auch gesungen werden darf nur im Freien oder m...</td>\n",
       "      <td>Man hat gesungen.</td>\n",
       "      <td>['convert_passive_to_active']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6096</th>\n",
       "      <td>So sahen die Nummern-Tafeln nämlich vor der Um...</td>\n",
       "      <td>die Nummern-Tafeln hat So nämlich vor der Umst...</td>\n",
       "      <td>['normalize_verb_tense']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11012</th>\n",
       "      <td>Dabei waren auch Sachen von den National-Sozia...</td>\n",
       "      <td>auch Sachen ist Dabei von den National-Soziali...</td>\n",
       "      <td>['normalize_verb_tense']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1667 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   query  \\\n",
       "uid                                                        \n",
       "6973   Sie können den Tumor auch mit Medikamenten bek...   \n",
       "9309   Das war schon das 3. Mal in einer Woche, dass ...   \n",
       "3745          Im Fernsehen wird über die Wahl berichtet.   \n",
       "6153   In Hollywood \" wurde als \" Beste Komödie \" aus...   \n",
       "2759   Schwere Vorwürfe gegen Opern-Sänger Placido Do...   \n",
       "...                                                  ...   \n",
       "939    Bundes-Kanzler Sebastian Kurz von der ÖVP woll...   \n",
       "8991   Ab 1. Mai werden aber viele Sport-Plätze wiede...   \n",
       "12404  Auch gesungen werden darf nur im Freien oder m...   \n",
       "6096   So sahen die Nummern-Tafeln nämlich vor der Um...   \n",
       "11012  Dabei waren auch Sachen von den National-Sozia...   \n",
       "\n",
       "                                    final_simplification  \\\n",
       "uid                                                        \n",
       "6973   Sie können den Tumor auch mit Medikamenten bek...   \n",
       "9309   Das ist schon das 3 Mal in einer Woche, dass i...   \n",
       "3745                                  Man hat berichtet.   \n",
       "6153                              Man hat ausgezeichnet.   \n",
       "2759   Schwere Vorwürfe gegen Opern·Sänger Placido Do...   \n",
       "...                                                  ...   \n",
       "939    Bundes-Kanzler Sebastian Kurz hat von der ÖVP ...   \n",
       "8991   Ab 1 Mai werden aber viele Sport·Plätze wieder...   \n",
       "12404                                  Man hat gesungen.   \n",
       "6096   die Nummern-Tafeln hat So nämlich vor der Umst...   \n",
       "11012  auch Sachen ist Dabei von den National-Soziali...   \n",
       "\n",
       "                                           applied_rules  \n",
       "uid                                                       \n",
       "6973                            ['normalize_verb_tense']  \n",
       "9309   ['convert_word_to_number', 'normalize_verb_ten...  \n",
       "3745                       ['convert_passive_to_active']  \n",
       "6153                       ['convert_passive_to_active']  \n",
       "2759                                  ['split_compound']  \n",
       "...                                                  ...  \n",
       "939                             ['normalize_verb_tense']  \n",
       "8991   ['convert_word_to_number', 'split_compound', '...  \n",
       "12404                      ['convert_passive_to_active']  \n",
       "6096                            ['normalize_verb_tense']  \n",
       "11012                           ['normalize_verb_tense']  \n",
       "\n",
       "[1667 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825262a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a04cd2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the generation settings used during training.\n",
    "# generation_kwargs = {\n",
    "#     \"min_length\": -1,\n",
    "#     \"top_k\": 0.0,\n",
    "#     \"top_p\": 1.0,\n",
    "#     \"do_sample\": True,\n",
    "#     \"pad_token_id\": policy_tokenizer.eos_token_id,\n",
    "#     \"max_new_tokens\": 40,\n",
    "# }\n",
    "\n",
    "# # Define the function to score responses with your reward model.\n",
    "# def compute_rewards_from_rm(responses: list[str]) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Computes rewards for a list of strings using the trained regression reward model.\n",
    "#     \"\"\"\n",
    "#     with torch.no_grad():\n",
    "#         # Tokenize the responses for the reward model.\n",
    "#         inputs = reward_tokenizer(\n",
    "#             responses,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#         ).to(device)\n",
    "        \n",
    "#         # Get the reward model's output logits (the scalar reward values).\n",
    "#         rewards = reward_model(**inputs).logits.squeeze(-1)\n",
    "        \n",
    "#     return rewards\n",
    "\n",
    "# print(\"\\n--- Setup complete. You can now run your evaluation script. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13af101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- Evaluating and Logging Final Model Performance ---\")\n",
    "\n",
    "# # Take a small, fresh sample from your original dataframe for evaluation\n",
    "# # Using a different random_state ensures we get a different set of prompts\n",
    "\n",
    "# eval_df_small = eval_df.sample(n=50, random_state=49)\n",
    "# eval_dataset = Dataset.from_pandas(eval_df_small)\n",
    "\n",
    "# # Tokenize the evaluation prompts\n",
    "# # Note: We are not using the dataloader here, just tokenizing a small batch\n",
    "# eval_prompts = policy_tokenizer(\n",
    "#     eval_df[\"query\"].tolist(), # Use the pandas DataFrame here\n",
    "#     return_tensors=\"pt\",\n",
    "#     padding=True,\n",
    "#     truncation=True,\n",
    "#     max_length = 60\n",
    "# )\n",
    "# eval_queries_list = [q.to(ppo_trainer.accelerator.device) for q in eval_prompts['input_ids']] # Ensure tensors are on the correct device\n",
    "\n",
    "# # Generate responses with the FINAL trained model\n",
    "# # We use torch.no_grad() for efficiency as we are not training\n",
    "# with torch.no_grad():\n",
    "#     eval_response_tensors = ppo_trainer.generate(eval_queries_list, **generation_kwargs)\n",
    "\n",
    "# # THE FIX: Isolate the generated tokens before decoding\n",
    "# # ==============================================================================\n",
    "# generated_tokens = []\n",
    "# # Get the original prompts' attention mask to find their true, unpadded lengths\n",
    "# prompt_attention_mask = eval_prompts['attention_mask']\n",
    "\n",
    "# for i, response_tensor in enumerate(eval_response_tensors):\n",
    "#     # Find the actual length of the prompt by summing its attention mask\n",
    "#     prompt_len = torch.sum(prompt_attention_mask[i])\n",
    "    \n",
    "#     # Slice the individual response tensor to get only the newly generated part\n",
    "#     generated_part = response_tensor[prompt_len:]\n",
    "#     generated_tokens.append(generated_part)\n",
    "\n",
    "# # Decode only the generated part\n",
    "# eval_responses = policy_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "# # ==============================================================================\n",
    "\n",
    "# # Score the final responses with the reward model\n",
    "# eval_rewards = compute_rewards_from_rm(eval_responses)\n",
    "\n",
    "# # Print the results in a clean format\n",
    "# print(\"\\n--- Final Model Generations ---\")\n",
    "# for i in range(len(eval_dataset[\"query\"])):\n",
    "#     print(f\"Query:    {eval_dataset['query'][i]}\")\n",
    "#     print(f\"Response: {eval_responses[i]}\")\n",
    "#     print(f\"Reward:   {eval_rewards[i].item():.4f}\")\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b0bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
