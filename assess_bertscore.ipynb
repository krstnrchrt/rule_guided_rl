{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089bcd84",
   "metadata": {},
   "source": [
    "Assess simplification output using BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, json, ast\n",
    "from collections import defaultdict, Counter\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6328f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.read_csv(\"bert_score_results.csv\")  # contains P, R, F1 per sentence\n",
    "df_rules  = pd.read_csv(\"master_data/output_assessment/ordered_simplifications_with_rules_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c26560",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d1fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before cleaning:\", len(df_rules))\n",
    "\n",
    "\n",
    "# Identify & remove faulty rows that only contain a number\n",
    "faulty = df_rules[\n",
    "    df_rules[\"applied_rules\"].str.contains(\"convert_word_to_number\") &\n",
    "    df_rules[\"final_simplification\"].str.match(r\"^\\d+$\")\n",
    "]\n",
    "print(\"Faulty rows:\", len(faulty))\n",
    "\n",
    "\n",
    "df_counter2 = df_rules.drop(faulty.index)\n",
    "\n",
    "\n",
    "print(\"After cleaning:\", len(df_counter2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f4ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge BERTScore results with rules dataframe on 'uid'\n",
    "df = df_rules.merge(\n",
    "    df_scores[[\"uid\", \"bertscore_precision\", \"bertscore_recall\", \"bertscore_f1\"]],\n",
    "    on=\"uid\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8732a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed 88 faulty rows\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2abb3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculation helper function\n",
    "def mean_ci(x, n_boot=2000, alpha=0.05, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    arr = np.array(x, dtype=float)\n",
    "    if len(arr) == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    boots = [np.mean(rng.choice(arr, size=len(arr), replace=True)) for _ in range(n_boot)]\n",
    "    lo, hi = np.percentile(boots, [100*alpha/2, 100*(1-alpha/2)])\n",
    "    return float(np.mean(arr)), float(np.std(arr)), float(lo), float(hi)\n",
    "\n",
    "#parsing applied rules\n",
    "def parse_rules(x):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return json.loads(x)\n",
    "        except Exception:\n",
    "            try:\n",
    "                return ast.literal_eval(x)\n",
    "            except Exception:\n",
    "                return []\n",
    "    return []\n",
    "\n",
    "df[\"applied_rules\"] = df[\"applied_rules\"].apply(parse_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a24f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect scores per rule\n",
    "rule_to_scores = defaultdict(lambda: {\"P\": [], \"R\": [], \"F1\": []})\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    for rule in row[\"applied_rules\"]:\n",
    "        if pd.notna(row[\"bertscore_precision\"]):\n",
    "            rule_to_scores[rule][\"P\"].append(row[\"bertscore_precision\"])\n",
    "            rule_to_scores[rule][\"R\"].append(row[\"bertscore_recall\"])\n",
    "            rule_to_scores[rule][\"F1\"].append(row[\"bertscore_f1\"])\n",
    "#Apply aggregation\n",
    "results = []\n",
    "for rule, vals in rule_to_scores.items():\n",
    "    meanP, stdP, loP, hiP = mean_ci(vals[\"P\"])\n",
    "    meanR, stdR, loR, hiR = mean_ci(vals[\"R\"])\n",
    "    meanF, stdF, loF, hiF = mean_ci(vals[\"F1\"])\n",
    "    results.append({\n",
    "        \"rule\": rule,\n",
    "        \"N\": len(vals[\"F1\"]),\n",
    "        \"mean_precision\": meanP, \"ci95_lo_P\": loP, \"ci95_hi_P\": hiP,\n",
    "        \"mean_recall\": meanR,    \"ci95_lo_R\": loR, \"ci95_hi_R\": hiR,\n",
    "        \"mean_f1\": meanF,        \"ci95_lo_F1\": loF, \"ci95_hi_F1\": hiF\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(\"mean_f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82522e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUle counts\n",
    "all_rules = [r for rules in df[\"applied_rules\"] for r in rules]\n",
    "rule_counts = Counter(all_rules)\n",
    "df_counts = pd.DataFrame(rule_counts.items(), columns=[\"rule\", \"count\"])\n",
    "\n",
    "# Merge counts into results\n",
    "df_final = df_results.merge(df_counts, on=\"rule\", how=\"left\").sort_values(\"mean_f1\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Average BERTScore aggregated per rule ===\")\n",
    "print(df_final[[\n",
    "    \"rule\", \"count\",\n",
    "    \"mean_precision\", #\"ci95_lo_P\", \"ci95_hi_P\",\n",
    "    \"mean_recall\",    #\"ci95_lo_R\", \"ci95_hi_R\",\n",
    "    \"mean_f1\",        #\"ci95_lo_F1\",\"ci95_hi_F1\"\n",
    "]].to_string(index=False, float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2a090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "im_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
