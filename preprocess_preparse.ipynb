{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88946266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24e8eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "\n",
    "from spacy.language import Language\n",
    "from spacy.pipeline import Sentencizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "656c19a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_postprocess_utils import *\n",
    "import importlib\n",
    "#importlib.reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ada37e",
   "metadata": {},
   "source": [
    "## 1) Preprocess Text Files:\n",
    "   1) Remove parentheses and their content.\n",
    "   2) Expand pre-defined abbreviations & acronyms.\n",
    "   3) Replace specific disallowed characters\n",
    "   4) Normalize special characters\n",
    "      1) only allow german letters (a–z, äöüß), digits (0–9), and basic punctuation like .?!:„“\n",
    "      2) Disallowed examples: $, §, <, >, , (comma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "836a556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_files(input_folder_path, output_path, abbr_json_path):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    # Load abbreviation dictionary\n",
    "    abbr_map = load_abbreviation_map(abbr_json_path)\n",
    "\n",
    "    # Process each text file in the input directory\n",
    "    for filename in os.listdir(input_folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_path = os.path.join(input_folder_path, filename)\n",
    "            output_filename = filename.replace(\".txt\", \"_preprocessed.txt\")\n",
    "            output_txt_path = os.path.join(output_path, output_filename)\n",
    "    \n",
    "\n",
    "            # Read the original text\n",
    "            with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "\n",
    "            # Apply transformations\n",
    "            text = remove_parentheses(text)\n",
    "            text = expand_abbreviations(text, abbr_map)\n",
    "            text = normalize_characters(text)  \n",
    "            text = character_substitution(text)\n",
    "\n",
    "            # Write the simplified text\n",
    "            with open(output_txt_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(text)\n",
    "\n",
    "            #print(f\"Processed: {filename} → {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e44e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_json_path = \"wiki_abkuerzungen.json\"\n",
    "input_folder_path = \"preprocessed_texts/apa-rst/0_original\"\n",
    "output_path = \"preprocessed_texts/apa-rst/1_preprocessed\"\n",
    "\n",
    "# Example usage\n",
    "preprocess_files(input_folder_path, output_path, abbr_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdae84b",
   "metadata": {},
   "source": [
    "## 2) Parsing and Tokenization using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0fc3ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.newline_sentencizer(doc)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a rule-based sentencizer BEFORE the parser to split on newlines too\n",
    "# Making sure that if headlines are present, they are separated and treated as single sentences when single/double newline is present\n",
    "@Language.component(\"newline_sentencizer\")\n",
    "def newline_sentencizer(doc):\n",
    "    # Split at each newline or double newline\n",
    "    start = 0\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.text == \"\\n\" or token.is_space and \"\\n\\n\" in token.text:\n",
    "            span = doc[start: i]\n",
    "            if span:\n",
    "                span[0].is_sent_start = True\n",
    "            start = i + 1\n",
    "    return doc\n",
    "\n",
    "#Add newline sentencizer\n",
    "nlp.add_pipe(\"newline_sentencizer\", before=\"parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d293ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_and_tokenization(input_folder_path, output_path):\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # Process each text file in the input directory\n",
    "    for filename in os.listdir(input_folder_path):\n",
    "        if filename.endswith(\"_preprocessed.txt\"):\n",
    "            input_path = os.path.join(input_folder_path, filename)\n",
    "            output_filename = filename.replace(\"_preprocessed.txt\", \"_parsed.txt\")\n",
    "            output_txt_path = os.path.join(output_path, output_filename)\n",
    "\n",
    "            # Read the preprocessed text\n",
    "            with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "\n",
    "            # Pre-segment into lines (headlines and paragraphs) & parse\n",
    "            lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "            docs = [nlp(line) for line in lines]\n",
    "\n",
    "            with open(output_txt_path, \"w\", encoding=\"utf-8\") as out:\n",
    "                for doc in docs:\n",
    "                    for sent in doc.sents:\n",
    "                        out.write(\"<s>\\n\")\n",
    "                        for token in sent:\n",
    "                            if token.is_space:\n",
    "                                continue\n",
    "\n",
    "                            out.write(\n",
    "                                f\"{token.text}\\t\"\n",
    "                                f\"{token.lemma_}\\t\"\n",
    "                                f\"{token.pos_}\\t\"\n",
    "                                f\"{token.dep_}\\t\"\n",
    "                                f\"{token.head.text}\\t\"\n",
    "                                f\"{token.morph}\\t\"\n",
    "                                f\"{token.is_stop}\\t\"\n",
    "                                f\"{token.ent_type_ or '-'}\\n\"\n",
    "                            )\n",
    "                        out.write(\"</s>\\n\\n\")\n",
    "                    \n",
    "            # Print the processed filename\n",
    "            print(f\"Parsed: {filename} → {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbd510d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'morphologizer', 'newline_sentencizer', 'parser', 'lemmatizer', 'attribute_ruler', 'ner']\n"
     ]
    }
   ],
   "source": [
    "#Checking the nlp pipeline order\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65bd6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = \"preprocessed_texts/apa-rst/1_preprocessed\"\n",
    "output_path = \"preprocessed_texts/apa-rst/2_parse_tokenize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42e70a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed: sent_1-29-11-21-or_preprocessed.txt → sent_1-29-11-21-or_parsed.txt\n",
      "Parsed: sent_5-freitag-28-1-22-or_preprocessed.txt → sent_5-freitag-28-1-22-or_parsed.txt\n",
      "Parsed: sent_5-dienstag-8-2-22-or_preprocessed.txt → sent_5-dienstag-8-2-22-or_parsed.txt\n",
      "Parsed: sent_1-21-2-18-or_preprocessed.txt → sent_1-21-2-18-or_parsed.txt\n",
      "Parsed: sent_5-18-1-22-or_preprocessed.txt → sent_5-18-1-22-or_parsed.txt\n",
      "Parsed: sent_2-29-11-21-or_preprocessed.txt → sent_2-29-11-21-or_parsed.txt\n",
      "Parsed: sent_4-21-2-18-or_preprocessed.txt → sent_4-21-2-18-or_parsed.txt\n",
      "Parsed: sent_3-freitag-28-1-22-or_preprocessed.txt → sent_3-freitag-28-1-22-or_parsed.txt\n",
      "Parsed: sent_3-dienstag-8-2-22-or_preprocessed.txt → sent_3-dienstag-8-2-22-or_parsed.txt\n",
      "Parsed: sent_4-29-11-21-or_preprocessed.txt → sent_4-29-11-21-or_parsed.txt\n",
      "Parsed: sent_1-18-1-22-or_preprocessed.txt → sent_1-18-1-22-or_parsed.txt\n",
      "Parsed: sent_5-21-2-18-or_preprocessed.txt → sent_5-21-2-18-or_parsed.txt\n",
      "Parsed: sent_4-18-1-22-or_preprocessed.txt → sent_4-18-1-22-or_parsed.txt\n",
      "Parsed: sent_4-freitag-28-1-22-or_preprocessed.txt → sent_4-freitag-28-1-22-or_parsed.txt\n",
      "Parsed: sent_3-21-2-18-or_preprocessed.txt → sent_3-21-2-18-or_parsed.txt\n",
      "Parsed: sent_4-dienstag-8-2-22-or_preprocessed.txt → sent_4-dienstag-8-2-22-or_parsed.txt\n",
      "Parsed: sent_2-18-1-22-or_preprocessed.txt → sent_2-18-1-22-or_parsed.txt\n",
      "Parsed: sent_5-29-11-21-or_preprocessed.txt → sent_5-29-11-21-or_parsed.txt\n",
      "Parsed: sent_2-freitag-28-1-22-or_preprocessed.txt → sent_2-freitag-28-1-22-or_parsed.txt\n",
      "Parsed: sent_3-18-1-22-or_preprocessed.txt → sent_3-18-1-22-or_parsed.txt\n",
      "Parsed: sent_3-29-11-21-or_preprocessed.txt → sent_3-29-11-21-or_parsed.txt\n",
      "Parsed: sent_2-21-2-18-or_preprocessed.txt → sent_2-21-2-18-or_parsed.txt\n",
      "Parsed: sent_2-dienstag-8-2-22-or_preprocessed.txt → sent_2-dienstag-8-2-22-or_parsed.txt\n",
      "Parsed: sent_1-freitag-28-1-22-or_preprocessed.txt → sent_1-freitag-28-1-22-or_parsed.txt\n",
      "Parsed: sent_1-01-trial_preprocessed.txt → sent_1-01-trial_parsed.txt\n",
      "Parsed: sent_1-dienstag-8-2-22-or_preprocessed.txt → sent_1-dienstag-8-2-22-or_parsed.txt\n"
     ]
    }
   ],
   "source": [
    "parsing_and_tokenization(input_folder_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c94272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b35b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # old version which worked\n",
    "# def parsing_and_tokenization(input_folder_path, output_path):\n",
    "\n",
    "#     # Ensure output directory exists\n",
    "#     os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "#     # Process each text file in the input directory\n",
    "#     for filename in os.listdir(input_folder_path):\n",
    "#         if filename.endswith(\"_preprocessed.txt\"):\n",
    "#             input_path = os.path.join(input_folder_path, filename)\n",
    "#             output_filename = filename.replace(\"_preprocessed.txt\", \"_parsed.txt\")\n",
    "#             output_txt_path = os.path.join(output_path, output_filename)\n",
    "\n",
    "#             # Read the preprocessed text\n",
    "#             with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#                 text = file.read()\n",
    "\n",
    "#             # Parse and tokenize the text\n",
    "#             # doc = nlp(text)\n",
    "\n",
    "#             # Pre-segment into lines (headlines and paragraphs) & parse\n",
    "#             lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "#             docs = [nlp(line) for line in lines]\n",
    "\n",
    "#             with open(output_txt_path, \"w\", encoding=\"utf-8\") as out:\n",
    "#                 for doc in docs:\n",
    "#                     out.write(\"<s>\\n\")\n",
    "\n",
    "#                     for token in doc:\n",
    "#                         if token.pos_ == \"SPACE\":\n",
    "#                             continue\n",
    "                    \n",
    "#                         norm_text, lemma, pos, dep, head = postprocess_token(token)\n",
    "#                         morph = str(token.morph)\n",
    "#                         is_stop = str(token.is_stop)\n",
    "#                         ent_type = token.ent_type_ if token.ent_type_ else \"-\"\n",
    "\n",
    "#                         out.write(f\"{norm_text}\\t{lemma}\\t{pos}\\t{dep}\\t{head}\\t{morph}\\t{is_stop}\\t{ent_type}\\n\")\n",
    "\n",
    "#                     out.write(\"</s>\\n\\n\")\n",
    "                    \n",
    "#             # Print the processed filename\n",
    "#             print(f\"Parsed: {filename} → {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c05f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630647ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6e7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "im_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
