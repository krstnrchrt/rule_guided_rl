{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67eb78f",
   "metadata": {},
   "source": [
    "### Assess performance and decide which hand-crafted rules are meaning-preserving and thus safe to include in the reward model (RM) or PPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ef65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from bert_score import score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac338688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables - link to the corresponding log file\n",
    "LOG_PATH = \"simplification_logs/all_parsed_log_2025-09-14_12-38-08.csv\"\n",
    "# Output of final cleaned up file \n",
    "OUTPUT_FILENAME = 'master_data/output_assessment/ordered_simplifications_with_rules_clean.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d5cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99543474",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83df4e5",
   "metadata": {},
   "source": [
    "#### There are non-null rows in simplified, identified to come from word_to_number() vconversion. They need to be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d87e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='any', axis=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb9b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b243395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"master_data/output_assessment/all_simplifications.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb22725",
   "metadata": {},
   "source": [
    "# Filter out and aggregate from simplification log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a74d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f05351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rule categories\n",
    "PARTIAL_RULES = {\"split_compound\", \"convert_word_to_number\"}\n",
    "SPLIT_RULES = {\"rewrite_apposition\", \"simplify_subordinate\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s: str) -> str:\n",
    "    \"\"\"Whitespace-normalize a string for duplicate checks.\"\"\"\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def split_into_sentences(text: str):\n",
    "    \"\"\"\n",
    "    Very lightweight sentence splitter for cleanup/dedup.\n",
    "    Splits on . ! ? while keeping punctuation.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    parts = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "    return [normalize(p) for p in parts if normalize(p)]\n",
    "\n",
    "def dedup_preserve_order(items):\n",
    "    \"\"\"Remove duplicates while preserving order.\"\"\"\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for it in items:\n",
    "        key = normalize(it)\n",
    "        if key not in seen:\n",
    "            out.append(it)\n",
    "            seen.add(key)\n",
    "    return out\n",
    "\n",
    "records = []\n",
    "\n",
    "# Process sentence-by-sentence groups\n",
    "for uid, group in df.groupby(\"uid\", sort=False):\n",
    "    group = group.reset_index(drop=True)\n",
    "\n",
    "    # Collect applied rules\n",
    "    applied_rules_list = group[group[\"applied\"] == True][\"rule\"].tolist()\n",
    "    if not applied_rules_list:\n",
    "        continue\n",
    "\n",
    "    # Deduplicate rules while preserving order\n",
    "    seen_rules = set()\n",
    "    unique_applied_rules = []\n",
    "    for r in applied_rules_list:\n",
    "        if r not in seen_rules:\n",
    "            unique_applied_rules.append(r)\n",
    "            seen_rules.add(r)\n",
    "\n",
    "    # Did any split-type rule fire?\n",
    "    split_applied = any(r in SPLIT_RULES for r in unique_applied_rules)\n",
    "\n",
    "    # Start from the true original\n",
    "    main_sentence = normalize(group[\"initial_original_sentence\"].iloc[0])\n",
    "    sentences = [main_sentence]\n",
    "    seen_sentences = {main_sentence}\n",
    "\n",
    "    # Replay transformations\n",
    "    for _, row in group.iterrows():\n",
    "        if not row[\"applied\"]:\n",
    "            continue\n",
    "\n",
    "        rule = row[\"rule\"]\n",
    "        simplified_piece = normalize(row[\"simplified\"]) if pd.notna(row[\"simplified\"]) else \"\"\n",
    "        original_piece   = normalize(row[\"original\"]) if pd.notna(row[\"original\"]) else \"\"\n",
    "\n",
    "        if rule in PARTIAL_RULES:\n",
    "            # Patch fragment into the last sentence\n",
    "            if original_piece and original_piece in sentences[-1]:\n",
    "                sentences[-1] = sentences[-1].replace(original_piece, simplified_piece, 1)\n",
    "\n",
    "        elif rule in SPLIT_RULES:\n",
    "            # Append new sentence(s), deduped\n",
    "            if simplified_piece:\n",
    "                new_sents = split_into_sentences(simplified_piece) or [simplified_piece]\n",
    "                for ns in new_sents:\n",
    "                    ns_norm = normalize(ns)\n",
    "                    if ns_norm not in seen_sentences:\n",
    "                        sentences.append(ns)\n",
    "                        seen_sentences.add(ns_norm)\n",
    "\n",
    "        else:\n",
    "            # Full-sentence rewrite\n",
    "            if simplified_piece:\n",
    "                sentences[-1] = simplified_piece\n",
    "\n",
    "    # --- Post-processing\n",
    "    sentences = dedup_preserve_order(sentences)\n",
    "\n",
    "    original_raw  = group[\"initial_original_sentence\"].iloc[0]\n",
    "    original_norm = normalize(original_raw)\n",
    "\n",
    "    if split_applied:\n",
    "        # Check if the original still appears exactly as one of the collected sentences\n",
    "        has_exact_original = any(normalize(s) == original_norm for s in sentences)\n",
    "        has_transformed    = any(original_norm in normalize(s) and normalize(s) != original_norm for s in sentences)\n",
    "\n",
    "        if has_exact_original and not has_transformed:\n",
    "            # Only drop the original if it is *unchanged* and other sentences exist\n",
    "            if len(sentences) > 1:\n",
    "                sentences = [s for s in sentences if normalize(s) != original_norm]\n",
    "\n",
    "    # Join final sentences\n",
    "    final_text = \" \".join(sentences).strip()\n",
    "\n",
    "\n",
    "    # Store result\n",
    "    records.append({\n",
    "        \"uid\": uid,\n",
    "        \"original_sentence\": group[\"initial_original_sentence\"].iloc[0],\n",
    "        \"final_simplification\": final_text,\n",
    "        \"applied_rules\": unique_applied_rules\n",
    "    })\n",
    "\n",
    "# Build DataFrame\n",
    "result_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c515ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6901aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the final result by UID to approximate the original file order\n",
    "result_df = result_df.sort_values(by='uid').reset_index(drop=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32644fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2814128",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanup = result_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d046b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanup.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c24d9",
   "metadata": {},
   "source": [
    "### Apply last step cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd09ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cleanup.columns = df_cleanup.columns.str.strip() # This removes leading/trailing spaces from each column name\n",
    "\n",
    "def clean_all_whitespace(sentence):\n",
    "  \"\"\"\n",
    "  Replaces multiple spaces inside a string with a single space,\n",
    "  and then strips leading/trailing whitespace.\n",
    "  \"\"\"\n",
    "  # 0: If the input is not a string, return it as is\n",
    "  if not isinstance(sentence, str):\n",
    "      return sentence\n",
    "  # 1: Clean up all internal whitespace first.\n",
    "  sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "  # 2: Strip whitespace from the beginning and end\n",
    "  sentence = re.sub(r'\\s+([.,:;?!])', r'\\1', sentence)\n",
    "  return sentence\n",
    "\n",
    "columns_to_clean = ['original_sentence', 'final_simplification']\n",
    "\n",
    "print(f\"Attempting to strip whitespace from columns: {', '.join(columns_to_clean)}\")\n",
    "\n",
    "# Loop through the identified columns and apply the strip() method\n",
    "for col in columns_to_clean:\n",
    "  if col in df_cleanup.columns and df_cleanup[col].dtype == 'object':\n",
    "    print(f\"Cleaning column: '{col}'...\")\n",
    "    # Apply our new, more powerful cleaning function to each sentence in the column\n",
    "    df_cleanup[col] = df_cleanup[col].apply(clean_all_whitespace)\n",
    "  else:\n",
    "    print(f\"Column '{col}' not found or is not a text column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleanup.head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f64756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanup.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanup.to_csv(OUTPUT_FILENAME, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302347d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSaved the final, ordered file: '{OUTPUT_FILENAME}'\")\n",
    "print(\"\\nHere is a preview of the new format:\")\n",
    "print(df_cleanup.head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f102f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanup.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a688f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keelp original_sentence and final_simplification to save in a different setting for assessmen\n",
    "final_pairs = df_cleanup[['uid', 'original_sentence', 'final_simplification']]\n",
    "final_pairs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pairs.to_csv(\"master_data/output_assessment/final_simplified_pairs_cleaned_FINAL.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056848c8",
   "metadata": {},
   "source": [
    "# Calculate BERT Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21de256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilize the final variable or import the saved file\n",
    "df_rules = df_cleanup.copy()\n",
    "\n",
    "#df_rules = pd.read_csv(OUTPUT_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation: original -> simplified\n",
    "\n",
    "originals = df_rules[\"original_sentence\"].tolist()\n",
    "simplifieds = df_rules[\"final_simplification\"].tolist()\n",
    "\n",
    "# Compute BERTScore using German-specific model\n",
    "P, R, F1 = score(simplifieds, originals, model_type=\"xlm-roberta-large\", lang=\"de\")\n",
    "\n",
    "# Add scores back to dataframe\n",
    "df_rules[\"bertscore_f1\"] = F1.tolist()\n",
    "\n",
    "# Save the results\n",
    "df_rules.to_csv(\"master_data/output_assessment/bert_score_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
